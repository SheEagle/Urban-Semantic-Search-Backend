# # app/services/search_service.py
# import io
# from typing import Optional, List
#
# import numpy as np
# from PIL import Image
#
# from backend.app.core.config import settings
# from backend.app.schema.search import SearchResultItem, SearchFilters, HeatmapPoint
# from backend.app.utils.global_state import GlobalState
#
# from qdrant_client import models
#
#
# class SearchService:
#     def __init__(self):
#         self.collection_name = settings.COLLECTION_NAME
#         self.MAP_COLLECTION = settings.MAP_COLLECTION
#         self.DOC_COLLECTION = settings.DOC_COLLECTION
#
#     def get_heatmap_data(self, query: str, limit: int = 2000) -> List[HeatmapPoint]:
#         """
#         è·å–è½»é‡çº§çƒ­åŠ›å›¾æ•°æ® (åªè¿”å› lat, lng, score)
#         """
#         client = GlobalState.get_db()
#         text_model = GlobalState.get_text_model()
#
#         points = []
#
#         # 1. ç”Ÿæˆæœç´¢å‘é‡
#         # å¦‚æœ query ä¸ºç©ºï¼Œç†è®ºä¸Šåº”è¯¥ç”¨ scroll è·å–å…¨é‡åˆ†å¸ƒï¼Œè¿™é‡Œå‡è®¾ä¸€å®šæœ‰ query
#         try:
#             vector = text_model.encode(query).tolist()
#         except Exception as e:
#             print(f"Embedding failed: {e}")
#             return []
#
#         # 2. å®šä¹‰åªè·å– location å­—æ®µçš„è¿‡æ»¤å™¨ (æ€§èƒ½å…³é”®!)
#         # è¿™æ · Qdrant ä¸ä¼šæŠŠå‡  MB çš„ full_metadata ä¼ å›æ¥
#         payload_selector = models.PayloadSelectorInclude(
#             include=["location", "geo_detail"]
#         )
#
#         # --- A. æœæ–‡æ¡£ (Venice Docs) ---
#         try:
#             # å‡è®¾ limit åˆ†ä¸€åŠç»™æ–‡æ¡£
#             doc_hits = client.query_points(
#                 collection_name=self.DOC_COLLECTION,
#                 # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šquery åªä¼ å‘é‡å€¼
#                 query=vector,
#
#                 # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šç”¨ using æŒ‡å®šå‘é‡åç§°
#                 using="text_vector",
#                 limit=limit // 2,
#                 with_payload=payload_selector,  # ğŸ”¥ åªå–åæ ‡
#                 score_threshold=0.35  # è¿‡æ»¤æ‰å®Œå…¨ä¸ç›¸å…³çš„
#             )
#
#             if hasattr(doc_hits, 'points'):
#                 doc_hits = doc_hits.points
#             elif isinstance(doc_hits, tuple) and doc_hits[0] == 'points':
#                 doc_hits = doc_hits[1]
#
#             for i, hit in enumerate(doc_hits):
#                 if isinstance(hit, tuple): continue
#                 if not hasattr(hit, 'score'): continue
#
#                 loc = hit.payload.get('location')
#                 # é˜²å¾¡æ€§æ£€æŸ¥
#                 if loc and 'lat' in loc and 'lon' in loc:
#                     points.append(HeatmapPoint(
#                         lat=loc['lat'],
#                         lng=loc['lon'],
#                         score=hit.score
#                     ))
#
#         except Exception as e:
#             print(f"Heatmap doc search error: {e}")
#
#         # --- B. æœåœ°å›¾ (Venice Maps) ---
#         # æ³¨æ„ï¼šåœ°å›¾éœ€è¦ text-to-image æœç´¢ï¼Œè¿™é‡Œå‡è®¾ä½ å·²ç»æŠŠ map_tiles å­˜å…¥äº† text å‘é‡ç©ºé—´
#         # æˆ–è€…ä½ ç”¨çš„æ˜¯ CLIP çš„ text encoder æœ image vector
#         # è¿™é‡Œå‡è®¾ä½¿ç”¨ç»Ÿä¸€çš„ text_model æœç´¢
#         pe_model = GlobalState.get_pe_model()  # ğŸ”¥ è·å– PE/CLIP
#
#         vector_np = pe_model.extract_text_features(query)
#         # å¤„ç†å¯èƒ½çš„ç»´åº¦é—®é¢˜ (1, 512) -> [512]
#         if hasattr(vector_np, 'tolist'):
#             vector_list = vector_np.tolist()
#         else:
#             vector_list = vector_np
#
#         if isinstance(vector_list[0], list):
#             vector_list = vector_list[0]
#
#         # 3. æœç´¢ venice_maps
#
#         try:
#             map_hits = client.query_points(
#                 collection_name=self.MAP_COLLECTION,
#                 query=vector_list,  # ğŸ”¥ æŒ‡å®š pe_vector
#                 limit=limit // 2,
#                 with_payload=payload_selector,
#                 score_threshold=0.15
#             )
#
#             if hasattr(map_hits, 'points'):
#                 map_hits = map_hits.points
#             elif isinstance(map_hits, tuple) and map_hits[0] == 'points':
#                 map_hits = map_hits[1]
#
#             for i, hit in enumerate(map_hits):
#                 if isinstance(hit, tuple): continue
#                 if not hasattr(hit, 'score'): continue
#
#                 loc = hit.payload.get('location')
#
#                 # å¦‚æœ location æ²¡ç›´æ¥åœ¨ payload é‡Œï¼Œå¯èƒ½è¦ç®—ä¸€ä¸‹ (é’ˆå¯¹ Map Tile)
#                 if not loc:
#                     # å°è¯•ä» geo_detail ç®—ä¸­å¿ƒç‚¹
#                     geo = hit.payload.get('geo_detail', {}).get('wgs84', {})
#                     if 'center' in geo:
#                         loc = {'lat': geo['center'][0], 'lon': geo['center'][1]}
#
#                 if loc and 'lat' in loc and 'lon' in loc:
#                     points.append(HeatmapPoint(
#                         lat=loc['lat'],
#                         lng=loc['lon'],
#                         score=hit.score * 1.2  # ç»™åœ°å›¾ä¸€ç‚¹åŠ æƒï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å°‘
#                     ))
#
#         except Exception as e:
#             print(f"Heatmap map search error: {e}")
#
#         return points
#
#     def _normalize_scores(self, results: List[SearchResultItem]) -> List[SearchResultItem]:
#         """
#         å¯¹æœç´¢ç»“æœçš„åˆ†æ•°è¿›è¡Œ Z-Score å½’ä¸€åŒ– (Standardization)ã€‚
#         å°†ä¸åŒåˆ†å¸ƒçš„åˆ†æ•°æ˜ å°„åˆ°å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒä¸Šã€‚
#         """
#         if not results:
#             return results
#
#         # 1. æå–æ‰€æœ‰åˆ†æ•°
#         scores = [r.score for r in results]
#
#         # 2. è®¡ç®—ç»Ÿè®¡é‡
#         mean = np.mean(scores)
#         std = np.std(scores)
#
#         # 3. é˜²å¾¡æ€§å¤„ç†ï¼šå¦‚æœæ ‡å‡†å·®ä¸º0ï¼ˆä¾‹å¦‚åªæœ‰ä¸€ä¸ªç»“æœï¼Œæˆ–æ‰€æœ‰åˆ†æ•°ç›¸åŒï¼‰
#         if std == 0:
#             # è¿™ç§æƒ…å†µä¸‹æ— æ³•è¿›è¡Œ Z-Scoreï¼Œå¯ä»¥é€‰æ‹©ä¸å¤„ç†ï¼Œæˆ–è€…å½’ä¸€åŒ–ä¸º 0
#             # è¿™é‡Œé€‰æ‹©ä¿æŒåŸæ ·ï¼Œæˆ–è€…ä½ å¯ä»¥æ‰‹åŠ¨è®¾ä¸º 1.0 (å¦‚æœåˆ†æ•°éƒ½å¾ˆé«˜)
#             return results
#
#         # 4. æ‰§è¡Œå½’ä¸€åŒ–
#         for r in results:
#             # æ–°åˆ†æ•° = (æ—§åˆ†æ•° - å‡å€¼) / æ ‡å‡†å·®
#             # æ³¨æ„ï¼šè¿™æ ·å¤„ç†åï¼Œåˆ†æ•°ä¼šæœ‰æ­£æœ‰è´Ÿ
#             r.score = (r.score - mean) / std
#
#         return results
#
#     def _build_qdrant_filters(self, filters: SearchFilters) -> Optional[models.Filter]:
#         """
#         è¾…åŠ©å‡½æ•°ï¼šå°† Pydantic è¿‡æ»¤å™¨è½¬æ¢ä¸º Qdrant Filter å¯¹è±¡
#         """
#         if not filters:
#             return None
#
#         conditions = []
#
#         # 1. å¹´ä»½èŒƒå›´è¿‡æ»¤ (Payload ä¸­å¿…é¡»æœ‰ 'year' å­—æ®µ)
#         if filters.year_start is not None:
#             conditions.append(
#                 models.FieldCondition(
#                     key="year",
#                     range=models.Range(gte=filters.year_start)
#                 )
#             )
#         if filters.year_end is not None:
#             conditions.append(
#                 models.FieldCondition(
#                     key="year",
#                     range=models.Range(lte=filters.year_end)
#                 )
#             )
#
#         # 2. ç‰¹å®šåœ°å›¾æ¥æºè¿‡æ»¤ (Payload ä¸­å¿…é¡»æœ‰ 'source_image' å­—æ®µ)
#         if filters.map_source:
#             conditions.append(
#                 models.FieldCondition(
#                     key="source_image",
#                     match=models.MatchValue(value=filters.map_source)
#                 )
#             )
#
#         # 3. åæ ‡èŒƒå›´è¿‡æ»¤ (Payload ä¸­å¿…é¡»æœ‰ 'location' Geo å­—æ®µ)
#         # å‡è®¾ bbox æ ¼å¼ä¸º [min_lon, min_lat, max_lon, max_lat]
#         if filters.geo_bbox and len(filters.geo_bbox) == 4:
#             conditions.append(
#                 models.FieldCondition(
#                     key="location",  # Qdrant ä¸­çš„ Payload å­—æ®µå
#                     geo_bounding_box=models.GeoBoundingBox(
#                         bottom_right=models.GeoPoint(
#                             lon=filters.geo_bbox[2],
#                             lat=filters.geo_bbox[1]
#                         ),
#                         top_left=models.GeoPoint(
#                             lon=filters.geo_bbox[0],
#                             lat=filters.geo_bbox[3]
#                         )
#                     )
#                 )
#             )
#
#         if not conditions:
#             return None
#
#         return models.Filter(must=conditions)
#
#         # ä¿®æ”¹é€šç”¨çš„æ‰§è¡Œæ–¹æ³•ï¼Œæ¥æ”¶ query_filter
#
#     # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ–‡æœæ–‡é€»è¾‘ ---
#     def _search_documents(self, query: str, limit: int, threshold: float, q_filter: models.Filter) -> List[
#         SearchResultItem]:
#         """
#         ä½¿ç”¨ MiniLM æ¨¡å‹æœç´¢ venice_docs é›†åˆ (æ–‡æœæ–‡)
#         """
#         client = GlobalState.get_db()
#         text_model = GlobalState.get_text_model()  # ğŸ”¥ è·å– MiniLM
#
#         # 1. ç”Ÿæˆè¯­ä¹‰å‘é‡
#         vector = text_model.encode(query).tolist()
#
#         # 2. æœç´¢ venice_docs
#         hits = client.query_points(
#             collection_name=self.DOC_COLLECTION,
#             # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šquery åªä¼ å‘é‡å€¼
#             query=vector,
#
#             # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šç”¨ using æŒ‡å®šå‘é‡åç§°
#             using="text_vector",
#             query_filter=q_filter,
#             limit=limit,
#             with_payload=True
#         )
#
#         if hasattr(hits, 'points'):
#             hits = hits.points
#         elif isinstance(hits, tuple) and hits[0] == 'points':
#             hits = hits[1]
#
#         results = []
#
#         for i, hit in enumerate(hits):
#             if isinstance(hit, tuple): continue
#             if not hasattr(hit, 'score'): continue
#             if hit.score < threshold: continue
#
#             payload = hit.payload or {}
#             loc = payload.get('location', {})
#
#             results.append(SearchResultItem(
#                 id=str(hit.id),
#                 score=hit.score,
#                 year=payload.get('year', 0),
#                 lat=loc.get('lat', 0.0),
#                 lng=loc.get('lon', 0.0),
#                 source_dataset=payload.get('source_dataset', 'Document'),
#                 content=payload.get('content', '')[:200] + "...",  # æˆªå–æ‘˜è¦
#                 fullData=payload,
#                 type="document",  # ğŸ”¥ æ ‡è®°ä¸ºæ–‡æ¡£
#                 pixel_coords=None
#             ))
#         return results
#
#         # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ–‡æœå›¾é€»è¾‘ (åŸæœ‰çš„é€»è¾‘å¾®è°ƒ) ---
#
#     def _search_maps_by_text(self, query: str, limit: int, threshold: float, q_filter: models.Filter) -> List[
#         SearchResultItem]:
#         """
#         ä½¿ç”¨ PE æ¨¡å‹æœç´¢ venice_maps é›†åˆ (æ–‡æœå›¾)
#         """
#         client = GlobalState.get_db()
#         pe_model = GlobalState.get_pe_model()  # ğŸ”¥ è·å– PE/CLIP
#
#         # 1. ç¿»è¯‘ (å¯é€‰ï¼Œå»ºè®®åŠ ä¸Š)
#         # try:
#         #     query = GoogleTranslator(source='auto', target='en').translate(query)
#         # except: pass
#
#         # 2. ç”Ÿæˆè§†è§‰å¯¹é½å‘é‡
#         # extract_text_features è¿”å› numpy array
#         vector_np = pe_model.extract_text_features(query)
#         # å¤„ç†å¯èƒ½çš„ç»´åº¦é—®é¢˜ (1, 512) -> [512]
#         if hasattr(vector_np, 'tolist'):
#             vector_list = vector_np.tolist()
#         else:
#             vector_list = vector_np
#
#         if isinstance(vector_list[0], list):
#             vector_list = vector_list[0]
#
#         # 3. æœç´¢ venice_maps
#         hits = client.query_points(
#             collection_name=self.MAP_COLLECTION,
#             query=vector_list,  # ğŸ”¥ æŒ‡å®š pe_vector
#             query_filter=q_filter,
#             limit=limit,
#             with_payload=True
#         )
#
#         if hasattr(hits, 'points'):
#             hits = hits.points
#         elif isinstance(hits, tuple) and hits[0] == 'points':
#             hits = hits[1]
#
#         results = []
#
#         for i, hit in enumerate(hits):
#             if isinstance(hit, tuple): continue
#             if not hasattr(hit, 'score'): continue
#             if hit.score < threshold: continue
#
#             payload = hit.payload or {}
#             loc = payload.get('location', {})
#
#             results.append(SearchResultItem(
#                 id=str(hit.id),
#                 score=hit.score,
#                 year=payload.get('year', 0),
#                 lat=loc.get('lat', 0.0),
#                 lng=loc.get('lon', 0.0),
#                 pixel_coords=payload.get('pixel_coords'),
#                 image_source=payload.get('source_image'),
#                 content=f"Map Fragment ({payload.get('year', '')})",
#                 fullData=payload,
#                 type="map_tile"  # ğŸ”¥ æ ‡è®°ä¸ºåœ°å›¾åˆ‡ç‰‡
#             ))
#         return results
#
#     # --- ä¸»å…¥å£ï¼šæ–‡æœ¬æœç´¢ ---
#     def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
#         SearchResultItem]:
#         q_filter = self._build_qdrant_filters(filters)
#
#         # å®šä¹‰ä¸¤ä¸ªæ¨¡å‹å„è‡ªçš„â€œåŠæ ¼çº¿â€
#         # ç»éªŒå€¼ï¼šMiniLM ä½äº 0.4 é€šå¸¸æ˜¯ä¸ç›¸å…³çš„
#         DOC_MIN_SCORE = 0.35
#         # ç»éªŒå€¼ï¼šCLIP/PE ä½äº 0.15 é€šå¸¸æ˜¯éšæœºå™ªå£°
#         MAP_MIN_SCORE = 0.18
#         Z_SCORE_THRESHOLD = 0  # å‰”é™¤ä½äºå¹³å‡æ°´å¹³åŠä¸ªæ ‡å‡†å·®çš„ç»“æœ
#
#         doc_results = []
#         map_results = []
#
#         # 1. æœæ–‡æ¡£
#         try:
#             # å…ˆæ‹¿å›æ¥å¤šä¸€ç‚¹
#             raw_docs = self._search_documents(query, limit * 2, 0, q_filter)
#             # ğŸ›¡ï¸ ç¬¬ä¸€é“é˜²çº¿ï¼šç»å¯¹é˜ˆå€¼è¿‡æ»¤
#             doc_results = [r for r in raw_docs if r.score > DOC_MIN_SCORE]
#         except Exception as e:
#             print(f"âš ï¸ Doc search failed: {e}")
#
#         # 2. æœåœ°å›¾
#         try:
#             raw_maps = self._search_maps_by_text(query, limit * 2, 0, q_filter)
#             # ğŸ›¡ï¸ ç¬¬ä¸€é“é˜²çº¿ï¼šç»å¯¹é˜ˆå€¼è¿‡æ»¤
#             map_results = [r for r in raw_maps if r.score > MAP_MIN_SCORE]
#         except Exception as e:
#             print(f"âš ï¸ Map search failed: {e}")
#
#         # --- å¦‚æœæŸä¸€æ–¹è¢«è¿‡æ»¤å®Œäº†ï¼Œå°±åªå‰©å¦ä¸€æ–¹ï¼Œé¿å…äº†å¼ºè¡Œæ‹‰é«˜ ---
#
#         # 3. Z-Score å½’ä¸€åŒ– (ç›¸å¯¹æ’åº)
#         if doc_results:
#             doc_results = self._normalize_scores(doc_results)
#
#         if map_results:
#             map_results = self._normalize_scores(map_results)
#
#         # 4. åˆå¹¶ä¸æ’åº
#         all_results = doc_results + map_results
#         final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
#
#         # --- E. æ’åºä¸æˆªæ–­ ---
#         final_results.sort(key=lambda x: x.score, reverse=True)
#         return final_results[:2 * limit]
#
#     # def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> \
#     #         List[SearchResultItem]:
#     #     """
#     #     èšåˆæœç´¢ï¼šåŒæ—¶æœæ–‡æ¡£å’Œåœ°å›¾
#     #     """
#     #     q_filter = self._build_qdrant_filters(filters)
#     #
#     #     results = []
#     #
#     #     # 1. æœæ–‡æ¡£ (æ–‡æœæ–‡)
#     #     try:
#     #         doc_results = self._search_documents(query, limit, threshold, q_filter)
#     #         results.extend(doc_results)
#     #     except Exception as e:
#     #         print(f"âš ï¸ Doc search failed: {e}")
#     #
#     #     # 2. æœåœ°å›¾ (æ–‡æœå›¾)
#     #     try:
#     #         map_results = self._search_maps_by_text(query, limit, threshold, q_filter)
#     #         results.extend(map_results)
#     #     except Exception as e:
#     #         print(f"âš ï¸ Map search failed: {e}")
#     #
#     #     # 3. ç»Ÿä¸€æ’åº (æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½)
#     #     results.sort(key=lambda x: x.score, reverse=True)
#     #
#     #     # 4. æˆªå– Top K
#     #     return results[:2 * limit]
#
#     def _execute_qdrant_search(self, vector_list: list, limit: int, threshold: float,
#                                query_filter: models.Filter = None):
#         client = GlobalState.get_db()
#
#         hits = client.query_points(
#             collection_name=self.collection_name,
#             query=vector_list,
#             limit=limit,
#             query_filter=query_filter
#         )
#
#         if hasattr(hits, 'points'):
#             hits = hits.points
#         elif isinstance(hits, tuple) and hits[0] == 'points':
#             hits = hits[1]
#
#         return self._process_hits(hits, threshold)
#
#     def _process_hits(self, hits: list, threshold: float):
#         results = []
#         # 6. ç»“æœå°è£… (é€»è¾‘åŒ search_textï¼Œå¯ä»¥æŠ½å–æˆä¸€ä¸ªç§æœ‰æ–¹æ³• _hits_to_results)
#         for i, hit in enumerate(hits):
#             if isinstance(hit, tuple): continue
#             if not hasattr(hit, 'score'): continue
#             if hit.score < threshold: continue
#
#             payload = hit.payload or {}
#             loc = payload.get('location', {})
#
#             item = SearchResultItem(
#                 id=str(hit.id),
#                 score=hit.score,
#                 lat=loc.get('lat', 0.0),
#                 lng=loc.get('lon', 0.0),
#                 pixel_coords=payload.get('pixel_coords', [0, 0]),
#                 image_source=payload.get('source_image'),
#                 geo_polygon=payload.get('geo_detail')
#             )
#             results.append(item)
#
#         return results
#
#     def search_image(self, image_data: bytes, limit: int, threshold: float) -> list[SearchResultItem]:
#         # 1. è·å–å•ä¾‹
#         client = GlobalState.get_db()
#         model = GlobalState.get_pe_model()
#
#         # 2. å›¾ç‰‡é¢„å¤„ç†
#         try:
#             image = Image.open(io.BytesIO(image_data))
#         except Exception as e:
#             raise ValueError(f"Invalid image file: {e}")
#
#         # 3. æå–ç‰¹å¾
#         # model.extract_image_features è¿”å›çš„æ˜¯ shape ä¸º (1, ç»´åº¦) çš„ numpy æ•°ç»„
#         feature_array = model.extract_image_features([image])
#
#         # 4. è½¬æ¢æ ¼å¼
#         # å–å‡ºç¬¬0ä¸ªå…ƒç´ ï¼ˆå› ä¸ºæˆ‘ä»¬åªä¼ äº†1å¼ å›¾ï¼‰ï¼Œå¹¶è½¬ä¸º python list
#         vector_list = feature_array[0].tolist()
#
#         # 5. Qdrant æœç´¢ (é€»è¾‘å®Œå…¨å¤ç”¨ text searchï¼Œå› ä¸ºéƒ½æ˜¯å‘é‡æœå‘é‡)
#         print(f"ğŸ–¼ï¸ [Service] Searching Image in '{self.collection_name}'...")
#         hits = client.query_points(
#             collection_name=self.collection_name,
#             query=vector_list,
#             limit=limit
#         )
#
#         if hasattr(hits, 'points'):
#             hits = hits.points
#         elif isinstance(hits, tuple) and hits[0] == 'points':
#             hits = hits[1]
#
#         results = []
#
#         # 6. ç»“æœå°è£… (é€»è¾‘åŒ search_textï¼Œå¯ä»¥æŠ½å–æˆä¸€ä¸ªç§æœ‰æ–¹æ³• _hits_to_results)
#         for i, hit in enumerate(hits):
#             if isinstance(hit, tuple): continue
#             if not hasattr(hit, 'score'): continue
#             if hit.score < threshold: continue
#
#             payload = hit.payload or {}
#             loc = payload.get('location', {})
#
#             item = SearchResultItem(
#                 id=str(hit.id),
#                 score=hit.score,
#                 lat=loc.get('lat', 0.0),
#                 lng=loc.get('lon', 0.0),
#                 pixel_coords=payload.get('pixel_coords', [0, 0]),
#                 image_source=payload.get('source_image'),
#                 geo_polygon=payload.get('geo_detail')
#             )
#             results.append(item)
#
#         return results
#
#     # def search_text(self, query: str, limit: int, threshold: float) -> list[SearchResultItem]:
#     #     # 1. è·å–å•ä¾‹
#     #     client = GlobalState.get_db()
#     #     model = GlobalState.get_model()
#     #
#     #     # 2. æ–‡æœ¬ç¼–ç  (è°ƒç”¨ utils)
#     #     # æ³¨æ„ï¼šè¿™é‡Œä¼šè¿”å› (1, dim) çš„ numpy array
#     #     raw_vector = model.extract_text_features(query)
#     #
#     #     # 3. æ ¼å¼è½¬æ¢ (Numpy -> List)
#     #     if hasattr(raw_vector, 'flatten'):
#     #         vector_list = raw_vector.flatten().tolist()
#     #     elif isinstance(raw_vector, list):
#     #         vector_list = raw_vector
#     #     else:
#     #         vector_list = raw_vector.tolist()
#     #
#     #     # 4. Qdrant æœç´¢
#     #     print(f"ğŸ” [Service] Searching in '{self.collection_name}'...")
#     #     hits = client.query_points(
#     #         collection_name=self.collection_name,
#     #         query=vector_list,
#     #         limit=limit
#     #     )
#     #
#     #     if hasattr(hits, 'points'):
#     #         hits = hits.points
#     #     elif isinstance(hits, tuple) and hits[0] == 'points':
#     #         # åº”å¯¹æç«¯æƒ…å†µï¼Œå¦‚æœå®ƒæœ¬èº«å°±æ˜¯ä¸ªå…ƒç»„
#     #         hits = hits[1]
#     #
#     #         # è°ƒè¯•æ‰“å°ï¼Œç¡®ä¿ç°åœ¨ hits æ˜¯ä¸ªåˆ—è¡¨
#     #     # if isinstance(hits, list) and len(hits) > 0:
#     #
#     #     results = []
#     #
#     #     # 3. éå†ç»“æœ
#     #     for i, hit in enumerate(hits):
#     #         # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šå†æ¬¡æ£€æŸ¥ hit æ˜¯å¦ä¸º tuple (åº”å¯¹ä¸€äº›å¥‡æ€ªçš„è¿­ä»£å™¨è¡Œä¸º)
#     #         if isinstance(hit, tuple):
#     #             # å¦‚æœæ­¤æ—¶ hit è¿˜æ˜¯å…ƒç»„ ('points', [...])ï¼Œè¯´æ˜æ‹†ç®±æ²¡æ‹†å¹²å‡€æˆ–è€…ç»“æ„åµŒå¥—äº†
#     #             # è¿™ç§æƒ…å†µä¸‹é€šå¸¸è·³è¿‡æˆ–è€…å°è¯•å–å€¼ï¼Œè¿™é‡Œæˆ‘ä»¬åšä¸ªæ—¥å¿—
#     #             print(f"âš ï¸ è·³è¿‡å¼‚å¸¸æ•°æ®ç»“æ„ (index {i}): {hit}")
#     #             continue
#     #
#     #         # æ­£å¸¸é€»è¾‘ï¼šhit åº”è¯¥æ˜¯ ScoredPoint å¯¹è±¡
#     #         if not hasattr(hit, 'score'):
#     #             print(f"âš ï¸ è·³è¿‡æ— æ•ˆç‚¹ (index {i}), æ—  score å±æ€§")
#     #             continue
#     #
#     #         if hit.score < threshold:
#     #             continue
#     #
#     #         payload = hit.payload or {}
#     #         loc = payload.get('location', {})
#     #
#     #         item = SearchResultItem(
#     #             id=str(hit.id),
#     #             score=hit.score,
#     #             lat=loc.get('lat', 0.0),
#     #             lng=loc.get('lon', 0.0),
#     #             pixel_coords=payload.get('pixel_coords', [0, 0]),
#     #             image_source=payload.get('source_image'),
#     #             geo_polygon=payload.get('geo_detail')
#     #         )
#     #         results.append(item)
#     #
#     #     return results
#
#     # def search_text(self, query: str, limit: int, threshold: float, filters: SearchFilters = None):
#     #     model = GlobalState.get_model()
#     #     vector_list = model.extract_text_features(query)[0].tolist()
#     #
#     #     # æ„å»ºè¿‡æ»¤å™¨
#     #     q_filter = self._build_qdrant_filters(filters)
#     #
#     #     return self._execute_qdrant_search(vector_list, limit, threshold, query_filter=q_filter)
#
#     def get_heatmap_points(self, query: str = None, year_start: int = None, year_end: int = None, limit: int = 10000):
#         client = GlobalState.get_db()
#         model = GlobalState.get_model()
#
#         # 1. æ„å»ºè¿‡æ»¤å™¨ (æ—¶é—´/åœ°å›¾æºç­‰)
#         # å¤ç”¨ä½ ä¹‹å‰å†™å¥½çš„ _build_qdrant_filters
#         filters_obj = SearchFilters(year_start=year_start, year_end=year_end)
#         q_filter = self._build_qdrant_filters(filters_obj)
#
#         heatmap_data = []
#
#         # --- åˆ†æ”¯ A: æœç´¢æ¨¡å¼ (æœ‰å…³é”®è¯) ---
#         if query:
#             # 1. æ–‡æœ¬è½¬å‘é‡
#             vector = model.extract_text_features(query)[0].tolist()
#
#             # 2. å‘é‡æœç´¢
#             hits = client.search(
#                 collection_name=self.collection_name,
#                 query_vector=vector,
#                 query_filter=q_filter,
#                 limit=limit,  # è¿™é‡Œ limit å¯ä»¥å¼€å¤§ä¸€ç‚¹
#                 with_payload=['location'],  # ğŸ”¥ å…³é”®ï¼šåªå– locationï¼Œä¸è¦å…¶ä»–å¤§å­—æ®µ
#                 with_vectors=False
#             )
#
#             for hit in hits:
#                 loc = hit.payload.get('location', {})
#                 if 'lat' in loc and 'lon' in loc:
#                     heatmap_data.append({
#                         "lat": loc['lat'],
#                         "lng": loc['lon'],
#                         "score": hit.score  # ç”¨ç›¸ä¼¼åº¦ä½œä¸ºçƒ­åŠ›æƒé‡
#                     })
#
#         # --- åˆ†æ”¯ B: å…¨é‡/æµè§ˆæ¨¡å¼ (æ— å…³é”®è¯) ---
#         else:
#             # ä½¿ç”¨ Scroll æ¥å£éå†æ•°æ®
#             # Qdrant çš„ scroll ä¸€æ¬¡æœ€å¤šè¿”å›å‡ åƒæ¡ï¼Œå¦‚æœæ•°æ®é‡æå¤§éœ€è¦å¾ªç¯ scroll
#             # è¿™é‡Œæ¼”ç¤ºç®€å•çš„ä¸€æ¬¡æ€§ scroll
#             response = client.scroll(
#                 collection_name=self.collection_name,
#                 scroll_filter=q_filter,
#                 limit=limit,
#                 with_payload=['location'],  # ğŸ”¥ å…³é”®ï¼šåªå– location
#                 with_vectors=False
#             )
#             points = response[0]  # response æ˜¯ (points, offset)
#
#             for point in points:
#                 loc = point.payload.get('location', {})
#                 if 'lat' in loc and 'lon' in loc:
#                     heatmap_data.append({
#                         "lat": loc['lat'],
#                         "lng": loc['lon'],
#                         "score": 1.0  # å…¨é‡æ¨¡å¼ä¸‹ï¼Œå¯†åº¦å³çƒ­åº¦ï¼Œæƒé‡è®¾ä¸º 1
#                     })
#
#         return heatmap_data
#
#
# # å¯¼å‡ºå®ä¾‹
# search_service = SearchService()


# app/services/search_service.py
# import io
# from typing import Optional, List, Union
#
# import numpy as np
# from PIL import Image
#
# from backend.app.core.config import settings
# from backend.app.schema.search import SearchResultItem, SearchFilters, HeatmapPoint
# from backend.app.utils.global_state import GlobalState
#
# from qdrant_client import models
#
#
# # from qdrant_client.http.models import PointStruct, ScoredPoint # æ ¹æ®ç‰ˆæœ¬å¯èƒ½éœ€è¦å¼•å…¥
#
#
# class SearchService:
#     def __init__(self):
#         # ä»é…ç½®ä¸­è¯»å–é›†åˆåç§°
#         self.MAP_COLLECTION = settings.MAP_COLLECTION  # åœ°å›¾åˆ‡ç‰‡é›†åˆ (visual vector)
#         self.DOC_COLLECTION = settings.DOC_COLLECTION  # å†å²æ–‡çŒ®é›†åˆ (text vector + pe_vector)
#
#     # ==========================================================================
#     #  æ ¸å¿ƒï¼šè¾…åŠ©æ–¹æ³• (Helpers)
#     # ==========================================================================
#
#     def _normalize_scores(self, results: List[SearchResultItem]) -> List[SearchResultItem]:
#         """
#         Z-Score å½’ä¸€åŒ–ï¼šå°†ä¸åŒåˆ†å¸ƒçš„åˆ†æ•°æ˜ å°„åˆ°ç»Ÿä¸€æ ‡å‡†ï¼Œä»¥ä¾¿æ··åˆæ’åºã€‚
#         """
#         if not results:
#             return results
#
#         scores = [r.score for r in results]
#         mean = np.mean(scores)
#         std = np.std(scores)
#
#         if std == 0:
#             return results
#
#         for r in results:
#             r.score = (r.score - mean) / std
#
#         return results
#
#     def _build_qdrant_filters(self, filters: SearchFilters) -> Optional[models.Filter]:
#         """
#         æ„å»º Qdrant è¿‡æ»¤å™¨ (æ—¶é—´ã€ç©ºé—´ã€æ¥æº)
#         """
#         if not filters:
#             return None
#
#         conditions = []
#
#         # 1. å¹´ä»½èŒƒå›´
#         if filters.year_start is not None:
#             conditions.append(models.FieldCondition(key="year", range=models.Range(gte=filters.year_start)))
#         if filters.year_end is not None:
#             conditions.append(models.FieldCondition(key="year", range=models.Range(lte=filters.year_end)))
#
#         # 2. åœ°å›¾æ¥æº
#         if filters.map_source:
#             conditions.append(
#                 models.FieldCondition(key="source_image", match=models.MatchValue(value=filters.map_source)))
#
#         # 3. ç©ºé—´èŒƒå›´ (BBox)
#         if filters.geo_bbox and len(filters.geo_bbox) == 4:
#             conditions.append(
#                 models.FieldCondition(
#                     key="location",
#                     geo_bounding_box=models.GeoBoundingBox(
#                         bottom_right=models.GeoPoint(lon=filters.geo_bbox[2], lat=filters.geo_bbox[1]),
#                         top_left=models.GeoPoint(lon=filters.geo_bbox[0], lat=filters.geo_bbox[3])
#                     )
#                 )
#             )
#
#         return models.Filter(must=conditions) if conditions else None
#
#     def _hits_to_results(self, hits, threshold: float, result_type: str, default_content: str = "") -> List[
#         SearchResultItem]:
#         """
#         é€šç”¨ç»“æœè½¬æ¢ï¼šå°† Qdrant Point è½¬æ¢ä¸º SearchResultItem
#         """
#         results = []
#
#         # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›çš„æ˜¯ tuple ç»“æ„ (points, offset)
#         if isinstance(hits, tuple):
#             hits = hits[0]
#         # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›çš„æ˜¯å¯¹è±¡ä¸”åŒ…å« points å±æ€§
#         if hasattr(hits, 'points'):
#             hits = hits.points
#
#         if not hits:
#             return results
#
#         for hit in hits:
#             # é˜²å¾¡æ€§è·³è¿‡
#             if isinstance(hit, tuple) or not hasattr(hit, 'score'):
#                 continue
#
#             if hit.score < threshold:
#                 continue
#
#             payload = hit.payload or {}
#             loc = payload.get('location', {})
#
#             # åŒºåˆ†å†…å®¹å±•ç¤º
#             if result_type == "document":
#                 # æ–‡æ¡£æ˜¾ç¤º content å­—æ®µæ‘˜è¦
#                 content_preview = payload.get('content', '')[:200] + "..."
#             else:
#                 # åœ°å›¾æ˜¾ç¤ºé¢„è®¾æ ‡é¢˜
#                 content_preview = f"{default_content} ({payload.get('year', 'Unknown')})"
#
#             item = SearchResultItem(
#                 id=str(hit.id),
#                 score=hit.score,
#                 year=payload.get('year', 0),
#                 lat=loc.get('lat', 0.0),
#                 lng=loc.get('lon', 0.0),
#                 source_dataset=payload.get('source_dataset') or payload.get('source_image') or 'Unknown',
#                 content=content_preview,
#                 fullData=payload,
#                 type=result_type,
#                 pixel_coords=payload.get('pixel_coords'),  # åœ°å›¾ç‰¹æœ‰
#                 geo_polygon=payload.get('geo_detail')  # åœ°å›¾ç‰¹æœ‰
#             )
#             results.append(item)
#
#         return results

# ==========================================================================
#  åŠŸèƒ½ 1: æ–‡æœ¬æœç´¢ (Text Search) - æ··åˆæœç´¢
# ==========================================================================

# def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
#     SearchResultItem]:
#     """
#     æ–‡æœ¬ -> (æ–‡æœæ–‡ MiniLM) + (æ–‡æœå›¾ PE/CLIP)
#     """
#     client = GlobalState.get_db()
#     q_filter = self._build_qdrant_filters(filters)
#
#     # é˜ˆå€¼è®¾å®š
#     DOC_MIN_SCORE = 0.35  # æ–‡æ¡£ç›¸å…³æ€§é˜ˆå€¼
#     MAP_MIN_SCORE = 0.18  # åœ°å›¾ç›¸å…³æ€§é˜ˆå€¼
#
#     doc_results = []
#     map_results = []
#
#     # --- A. æœæ–‡æ¡£ (MiniLM) ---
#     try:
#         text_model = GlobalState.get_text_model()
#         text_vec = text_model.encode(query).tolist()
#
#         hits_doc = client.query_points(
#             collection_name=self.DOC_COLLECTION,
#             query=text_vec,
#             using="text_vector",  # æ˜¾å¼æŒ‡å®šè¯­ä¹‰å‘é‡
#             query_filter=q_filter,
#             limit=limit * 2,  # å¤šå–ä¸€ç‚¹ç”¨äºè¿‡æ»¤
#             with_payload=True
#         )
#         doc_results = self._hits_to_results(hits_doc, DOC_MIN_SCORE, "document")
#     except Exception as e:
#         print(f"âš ï¸ Doc search (Text) failed: {e}")
#
#     # --- B. æœåœ°å›¾ (PE/CLIP) ---
#     try:
#         pe_model = GlobalState.get_pe_model()
#         # è·å–æ–‡æœ¬çš„è§†è§‰ç‰¹å¾å‘é‡
#         pe_vec = pe_model.extract_text_features(query)
#         # æ ¼å¼æ¸…æ´—
#         if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
#         if isinstance(pe_vec, list) and len(pe_vec) == 1 and isinstance(pe_vec[0], list):
#             pe_vec = pe_vec[0]
#
#         hits_map = client.query_points(
#             collection_name=self.MAP_COLLECTION,
#             query=pe_vec,
#             # maps é›†åˆé€šå¸¸ä½¿ç”¨é»˜è®¤å‘é‡ï¼Œå¦‚æœå®šä¹‰äº†åå­—éœ€åŠ  using="pe_vector"
#             query_filter=q_filter,
#             limit=limit * 2,
#             with_payload=True
#         )
#         # åœ°å›¾ç»“æœ
#         map_results = self._hits_to_results(hits_map, MAP_MIN_SCORE, "map_tile", "Map Fragment")
#     except Exception as e:
#         print(f"âš ï¸ Map search (Text) failed: {e}")
#
#     # --- C. å½’ä¸€åŒ–ä¸åˆå¹¶ ---
#     if doc_results: doc_results = self._normalize_scores(doc_results)
#     if map_results: map_results = self._normalize_scores(map_results)
#
#     all_results = doc_results + map_results
#     all_results.sort(key=lambda x: x.score, reverse=True)
#
#     return all_results[:limit]
#
# # ==========================================================================
# #  åŠŸèƒ½ 2: å›¾ç‰‡æœç´¢ (Image Search) - æ··åˆæœç´¢ (NEW!)
# # ==========================================================================
#
# def search_image(self, image_data: bytes, limit: int, threshold: float) -> List[SearchResultItem]:
#     """
#     å›¾ç‰‡ -> (å›¾æœå›¾ Maps) + (å›¾æœæ–‡ Docs)
#     """
#     client = GlobalState.get_db()
#     pe_model = GlobalState.get_pe_model()
#
#     # 1. å›¾ç‰‡é¢„å¤„ç†ä¸å‘é‡åŒ–
#     try:
#         image = Image.open(io.BytesIO(image_data))
#         # extract_image_features è¿”å› shape (1, dim)
#         feature_array = pe_model.extract_image_features([image])
#
#         # è½¬æ¢ä¸º list [dim]
#         vector_list = feature_array[0].tolist()
#     except Exception as e:
#         print(f"âŒ Image processing failed: {e}")
#         raise ValueError(f"Invalid image: {e}")
#
#     # é˜ˆå€¼è®¾å®š (å›¾ç‰‡æœç´¢é€šå¸¸ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œé˜ˆå€¼å¯ä»¥ç¨é«˜)
#     MAP_IMG_MIN_SCORE = 0.4
#     DOC_IMG_MIN_SCORE = 0.4
#
#     doc_results = []
#     map_results = []
#
#     print(f"ğŸ–¼ï¸ [Search] Start Image Search...")
#
#     # --- A. æœåœ°å›¾ (å›¾æœå›¾: Image -> Image Vector) ---
#     try:
#         hits_map = client.query_points(
#             collection_name=self.MAP_COLLECTION,
#             query=vector_list,
#             # map é›†åˆåªæœ‰è§†è§‰å‘é‡ï¼Œé€šå¸¸æ˜¯é»˜è®¤å‘é‡
#             limit=limit * 2,
#             with_payload=True
#         )
#         map_results = self._hits_to_results(hits_map, MAP_IMG_MIN_SCORE, "map_tile", "Visual Match Map")
#     except Exception as e:
#         print(f"âš ï¸ Map search (Image) failed: {e}")
#
#     # --- B. æœæ–‡æ¡£ (å›¾æœæ–‡: Image -> Text's PE Vector) ---
#     # å‰æï¼šDOC_COLLECTION åœ¨å…¥åº“æ—¶è®¡ç®—å¹¶å­˜å‚¨äº† 'pe_vector'
#     try:
#         hits_doc = client.query_points(
#             collection_name=self.DOC_COLLECTION,
#             query=vector_list,
#             using="pe_vector",  # ğŸ”¥ å…³é”®ï¼šæŒ‡å®šä½¿ç”¨æ–‡æ¡£çš„è§†è§‰å¯¹é½å‘é‡
#             limit=limit * 2,
#             with_payload=True
#         )
#         doc_results = self._hits_to_results(hits_doc, DOC_IMG_MIN_SCORE, "document")
#     except Exception as e:
#         # å¦‚æœæ–‡æ¡£é›†åˆé‡Œæ²¡æœ‰ pe_vectorï¼Œè¿™é‡Œä¼šæŠ¥é”™ï¼Œæ•æ‰å®ƒä¸å½±å“åœ°å›¾æœç´¢
#         print(f"âš ï¸ Doc search (Image) failed (Maybe 'pe_vector' missing?): {e}")
#
#     # --- C. å½’ä¸€åŒ–ä¸åˆå¹¶ ---
#     # å³ä½¿æ˜¯å›¾ç‰‡æœç´¢ï¼Œä¸åŒé›†åˆçš„ç›¸ä¼¼åº¦åˆ†å¸ƒä¹Ÿå¯èƒ½ä¸åŒï¼Œå»ºè®®å½’ä¸€åŒ–
#     if doc_results: doc_results = self._normalize_scores(doc_results)
#     if map_results: map_results = self._normalize_scores(map_results)
#
#     all_results = map_results + doc_results
#     all_results.sort(key=lambda x: x.score, reverse=True)
#
#     return all_results[:limit]
# import io
# from typing import Optional, List
#
# import numpy as np
# from PIL import Image
#
# from backend.app.core.config import settings
# from backend.app.schema.search import SearchResultItem, SearchFilters, HeatmapPoint
# from backend.app.utils.global_state import GlobalState
#
# from qdrant_client import models
#
# class SearchService:
#     def __init__(self):
#         self.MAP_COLLECTION = settings.MAP_COLLECTION
#         self.DOC_COLLECTION = settings.DOC_COLLECTION
#
#     # ==========================================================================
#     #  æ ¸å¿ƒç®—æ³•: å½’ä¸€åŒ–ä¸è¾…åŠ©å‡½æ•°
#     # ==========================================================================
#
#     def _normalize_scores(self, results: List[SearchResultItem]) -> List[SearchResultItem]:
#         """
#         Z-Score å½’ä¸€åŒ– (Standardization)
#         å…¬å¼: z = (x - Î¼) / Ïƒ
#         ä½œç”¨: å°†ä¸åŒæ¨¡å‹çš„åˆ†æ•°æ˜ å°„åˆ°åŒä¸€ä¸ªæ ‡å‡†æ­£æ€åˆ†å¸ƒä¸Šï¼Œä½¿å®ƒä»¬å¯ä»¥ç›¸äº’æ¯”è¾ƒã€‚
#         """
#         if not results or len(results) < 2:
#             return results
#
#         # 1. æå–åˆ†æ•°
#         scores = [r.score for r in results]
#         mean = np.mean(scores)
#         std = np.std(scores)
#
#         # 2. é˜²å¾¡æ€§å¤„ç†ï¼šå¦‚æœæ ‡å‡†å·®ä¸º0 (æ‰€æœ‰åˆ†æ•°éƒ½ä¸€æ ·)ï¼Œæ— æ³•å½’ä¸€åŒ–
#         if std == 0:
#             return results
#
#         # 3. æ‰§è¡Œå½’ä¸€åŒ–
#         for r in results:
#             r.score = (r.score - mean) / std
#
#         return results
#
#     def _build_qdrant_filters(self, filters: SearchFilters) -> Optional[models.Filter]:
#         """æ„å»º Qdrant è¿‡æ»¤å™¨"""
#         if not filters:
#             return None
#
#         conditions = []
#         if filters.year_start is not None:
#             conditions.append(models.FieldCondition(key="year", range=models.Range(gte=filters.year_start)))
#         if filters.year_end is not None:
#             conditions.append(models.FieldCondition(key="year", range=models.Range(lte=filters.year_end)))
#         if filters.map_source:
#             conditions.append(
#                 models.FieldCondition(key="source_image", match=models.MatchValue(value=filters.map_source)))
#         if filters.geo_bbox and len(filters.geo_bbox) == 4:
#             conditions.append(
#                 models.FieldCondition(
#                     key="location",
#                     geo_bounding_box=models.GeoBoundingBox(
#                         bottom_right=models.GeoPoint(lon=filters.geo_bbox[2], lat=filters.geo_bbox[1]),
#                         top_left=models.GeoPoint(lon=filters.geo_bbox[0], lat=filters.geo_bbox[3])
#                     )
#                 )
#             )
#         return models.Filter(must=conditions) if conditions else None
#
#     def _hits_to_results(self, hits, result_type: str, default_content: str = "") -> List[SearchResultItem]:
#         """å°† Qdrant è¿”å›çš„åŸå§‹ hits è½¬æ¢ä¸ºç»Ÿä¸€çš„æ•°æ®ç»“æ„"""
#         results = []
#         if isinstance(hits, tuple): hits = hits[0]
#         if hasattr(hits, 'points'): hits = hits.points
#         if not hits: return results
#
#         for hit in hits:
#             if isinstance(hit, tuple) or not hasattr(hit, 'score'): continue
#
#             payload = hit.payload or {}
#             loc = payload.get('location', {})
#
#             # å†…å®¹å±•ç¤ºé€»è¾‘
#             content_preview = payload.get('content', '')[
#                               :200] + "..." if result_type == "document" else f"{default_content} ({payload.get('year', 'Unknown')})"
#
#             item = SearchResultItem(
#                 id=str(hit.id),
#                 score=hit.score,
#                 year=payload.get('year', 0),
#                 lat=loc.get('lat', 0.0),
#                 lng=loc.get('lon', 0.0),
#                 source_dataset=payload.get('source_dataset') or payload.get('source_image') or 'Unknown',
#                 content=content_preview,
#                 fullData=payload,
#                 type=result_type,
#                 pixel_coords=payload.get('pixel_coords'),
#                 image_source=payload.get('source_image'),
#                 geo_polygon=payload.get('geo_detail')
#             )
#             results.append(item)
#         return results
#
#     # ==========================================================================
#     #  åŠŸèƒ½ 1: æ–‡æœ¬æ··åˆæœç´¢ (Text -> Text & Image)
#     # ==========================================================================
#
# def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> \
#         List[SearchResultItem]:
#     """
#     å®ç°é€»è¾‘ï¼š
#     1. åˆ†åˆ«è·å– Document (æ–‡æœæ–‡) å’Œ Map (æ–‡æœå›¾) ç»“æœã€‚
#     2. ä½¿ç”¨å„è‡ªçš„â€œç»å¯¹é˜ˆå€¼â€è¿‡æ»¤æ‰æ— å…³ç»“æœã€‚
#     3. å¯¹ä¸¤ç»„ç»“æœåˆ†åˆ«è¿›è¡Œ Z-Score å½’ä¸€åŒ–ã€‚
#     4. åˆå¹¶ç»“æœã€‚
#     5. ä½¿ç”¨â€œç›¸å¯¹é˜ˆå€¼â€ (Z-Score > 0) å†æ¬¡è¿‡æ»¤ï¼Œä¿ç•™é«˜äºå¹³å‡æ°´å¹³çš„ç»“æœã€‚
#     6. æ’åºå¹¶è¿”å›ã€‚
#     """
#     client = GlobalState.get_db()
#     q_filter = self._build_qdrant_filters(filters)
#
#     # --- é…ç½®å‚æ•° ---
#     DOC_MIN_SCORE = 0.35  # æ–‡æ¡£ç»å¯¹é˜ˆå€¼ (MiniLM)
#     MAP_MIN_SCORE = 0.18  # åœ°å›¾ç»å¯¹é˜ˆå€¼ (CLIP/PE)
#     Z_SCORE_THRESHOLD = -0.5  # ç›¸å¯¹é˜ˆå€¼ (æ ‡å‡†å·®)ï¼Œè®¾ä¸º 0 è¡¨ç¤ºåªå–å¹³å‡åˆ†ä»¥ä¸Šçš„ï¼Œ-0.5 è¡¨ç¤ºç¨å®½å®¹ä¸€ç‚¹
#
#     doc_results = []
#     map_results = []
#
#     # 1. æœæ–‡æ¡£ (MiniLM)
#     try:
#         text_model = GlobalState.get_text_model()
#         text_vec = text_model.encode(query).tolist()
#
#         hits_doc = client.query_points(
#             collection_name=self.DOC_COLLECTION,
#             query=text_vec,
#             using="text_vector",
#             query_filter=q_filter,
#             limit=limit * 2,  # å¤šå–ä¸€å€ç”¨äºåç»­ç­›é€‰
#             with_payload=True
#         )
#         raw_docs = self._hits_to_results(hits_doc, "document")
#         # ğŸ›¡ï¸ ç»å¯¹é˜ˆå€¼è¿‡æ»¤
#         doc_results = [r for r in raw_docs if r.score > DOC_MIN_SCORE]
#     except Exception as e:
#         print(f"âš ï¸ Doc search failed: {e}")
#
#     # 2. æœåœ°å›¾ (PE/CLIP)
#     try:
#         pe_model = GlobalState.get_pe_model()
#         pe_vec = pe_model.extract_text_features(query)
#         if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
#         if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
#
#         hits_map = client.query_points(
#             collection_name=self.MAP_COLLECTION,
#             query=pe_vec,
#             # maps é›†åˆé»˜è®¤å‘é‡å°±æ˜¯è§†è§‰å‘é‡
#             query_filter=q_filter,
#             limit=limit * 2,
#             with_payload=True
#         )
#         raw_maps = self._hits_to_results(hits_map, "map_tile", "Map Fragment")
#         # ğŸ›¡ï¸ ç»å¯¹é˜ˆå€¼è¿‡æ»¤
#         map_results = [r for r in raw_maps if r.score > MAP_MIN_SCORE]
#     except Exception as e:
#         print(f"âš ï¸ Map search failed: {e}")
#
#     # --- 3. ç‹¬ç«‹å½’ä¸€åŒ– (å…³é”®æ­¥éª¤) ---
#     # å¿…é¡»åˆ†å¼€å½’ä¸€åŒ–ï¼Œå› ä¸ºä¸¤ä¸ªæ¨¡å‹çš„åŸå§‹åˆ†æ•°åˆ†å¸ƒå®Œå…¨ä¸åŒ
#     if doc_results:
#         doc_results = self._normalize_scores(doc_results)
#
#     if map_results:
#         map_results = self._normalize_scores(map_results)
#
#     # --- 4. åˆå¹¶ä¸æœ€ç»ˆæ’åº ---
#     all_results = doc_results + map_results
#
#     # ğŸ›¡ï¸ ç›¸å¯¹é˜ˆå€¼è¿‡æ»¤ (Z-Score è¿‡æ»¤)
#     # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†å‰”é™¤åœ¨å„è‡ªæ¨¡å‹ä¸­è¡¨ç°éƒ½å¾ˆå·®çš„â€œé•¿å°¾â€ç»“æœ
#     final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
#
#     # æ’åº
#     final_results.sort(key=lambda x: x.score, reverse=True)
#
#     return final_results[:limit]
#
#     # ==========================================================================
#     #  åŠŸèƒ½ 2: å›¾ç‰‡æ··åˆæœç´¢ (Image -> Image & Text)
#     # ==========================================================================
#
# def search_image(self, image_data: bytes, limit: int, threshold: float) -> List[SearchResultItem]:
#     """
#     å›¾ç‰‡æœç´¢åŒæ ·åº”ç”¨ Z-Score é€»è¾‘
#     """
#     client = GlobalState.get_db()
#     pe_model = GlobalState.get_pe_model()
#
#     try:
#         image = Image.open(io.BytesIO(image_data))
#         vector_list = pe_model.extract_image_features([image])[0].tolist()
#     except Exception as e:
#         raise ValueError(f"Invalid image: {e}")
#
#     # å›¾ç‰‡æœç´¢é€šå¸¸ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œé˜ˆå€¼å¯ä»¥é«˜ä¸€ç‚¹
#     MAP_IMG_MIN_SCORE = 0.45
#     DOC_IMG_MIN_SCORE = 0.40
#     Z_SCORE_THRESHOLD = -1.0  # å›¾ç‰‡æœç´¢ç»“æœè¾ƒå°‘ï¼Œç¨å¾®å®½å®¹ä¸€ç‚¹
#
#     doc_results = []
#     map_results = []
#
#     # 1. æœåœ°å›¾ (å›¾æœå›¾)
#     try:
#         hits_map = client.query_points(
#             collection_name=self.MAP_COLLECTION,
#             query=vector_list,
#             limit=limit * 2,
#             with_payload=True
#         )
#         raw_maps = self._hits_to_results(hits_map, "map_tile", "Visual Match")
#         map_results = [r for r in raw_maps if r.score > MAP_IMG_MIN_SCORE]
#     except Exception as e:
#         print(f"âš ï¸ Image->Map search failed: {e}")
#
#     # 2. æœæ–‡æ¡£ (å›¾æœæ–‡ - éœ€æ–‡æ¡£åº“æœ‰ pe_vector)
#     try:
#         hits_doc = client.query_points(
#             collection_name=self.DOC_COLLECTION,
#             query=vector_list,
#             using="pe_vector",
#             limit=limit * 2,
#             with_payload=True
#         )
#         raw_docs = self._hits_to_results(hits_doc, "document")
#         doc_results = [r for r in raw_docs if r.score > DOC_IMG_MIN_SCORE]
#     except Exception as e:
#         print(f"âš ï¸ Image->Doc search failed: {e}")
#
#     # 3. å½’ä¸€åŒ–ä¸åˆå¹¶
#     if map_results: map_results = self._normalize_scores(map_results)
#     if doc_results: doc_results = self._normalize_scores(doc_results)
#
#     all_results = map_results + doc_results
#     final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
#
#     final_results.sort(key=lambda x: x.score, reverse=True)
#
#     return final_results[:limit]
#
#     # ==========================================================================
#     #  åŠŸèƒ½ 3: 3D çƒ­åŠ›å›¾æ•°æ®
#     # ==========================================================================
#
# def get_heatmap_data(self, query: str, limit: int = 2000) -> List[HeatmapPoint]:
#     """
#     ä¸ºäº†æ€§èƒ½ï¼Œçƒ­åŠ›å›¾æ•°æ®ä¸è¿›è¡Œå¤æ‚çš„å½’ä¸€åŒ–ï¼Œç›´æ¥è¿”å›åŸå§‹åˆ†æ•°å³å¯ï¼Œ
#     æˆ–è€…åªè¿›è¡Œç®€å•çš„ Min-Max ç¼©æ”¾ã€‚è¿™é‡Œä¿æŒåŸå§‹åˆ†æ•°ã€‚
#     """
#     client = GlobalState.get_db()
#     payload_selector = models.PayloadSelectorInclude(include=["location"])
#     points = []
#
#     # æœæ–‡æ¡£
#     try:
#         text_model = GlobalState.get_text_model()
#         vec = text_model.encode(query).tolist()
#         hits = client.query_points(
#             self.DOC_COLLECTION, query=vec, using="text_vector",
#             limit=limit // 2, with_payload=payload_selector, score_threshold=0.35
#         )
#         if hasattr(hits, 'points'): hits = hits.points
#         for h in hits:
#             loc = h.payload.get('location')
#             if loc: points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=h.score))
#     except:
#         pass
#
#     # æœåœ°å›¾
#     try:
#         pe_model = GlobalState.get_pe_model()
#         vec = pe_model.extract_text_features(query)[0].tolist()
#         hits = client.query_points(
#             self.MAP_COLLECTION, query=vec,
#             limit=limit // 2, with_payload=payload_selector, score_threshold=0.20
#         )
#         if hasattr(hits, 'points'): hits = hits.points
#         for h in hits:
#             loc = h.payload.get('location')
#             if not loc:
#                 # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ locï¼Œå°è¯•ä» geo_detail è·å–ä¸­å¿ƒç‚¹ (å¦‚æœæœ‰)
#                 pass
#             if loc: points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=h.score * 1.1))
#     except:
#         pass
#
#     return points
#
# # ==========================================================================
# #  åŠŸèƒ½ 3: çƒ­åŠ›å›¾ (Heatmap)
# # ==========================================================================
#
# def get_heatmap_data(self, query: str, limit: int = 2000) -> List[HeatmapPoint]:
#     """
#     è·å–è½»é‡çº§çƒ­åŠ›å›¾æ•°æ®
#     """
#     client = GlobalState.get_db()
#     points = []
#
#     # ä»…åŒ…å«ä½ç½®ä¿¡æ¯çš„ Payload ç­›é€‰å™¨ (é«˜æ€§èƒ½)
#     payload_selector = models.PayloadSelectorInclude(include=["location", "geo_detail"])
#
#     # 1. æœæ–‡æ¡£ (MiniLM)
#     try:
#         text_model = GlobalState.get_text_model()
#         text_vec = text_model.encode(query).tolist()
#
#         hits = client.query_points(
#             collection_name=self.DOC_COLLECTION,
#             query=text_vec,
#             using="text_vector",
#             limit=limit // 2,
#             with_payload=payload_selector,
#             score_threshold=0.35
#         )
#         if hasattr(hits, 'points'): hits = hits.points
#
#         for hit in hits:
#             loc = hit.payload.get('location')
#             if loc and 'lat' in loc:
#                 points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=hit.score))
#     except Exception as e:
#         print(f"Heatmap doc error: {e}")
#
#     # 2. æœåœ°å›¾ (PE)
#     try:
#         pe_model = GlobalState.get_pe_model()
#         pe_vec = pe_model.extract_text_features(query)
#         if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
#         if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
#
#         hits = client.query_points(
#             collection_name=self.MAP_COLLECTION,
#             query=pe_vec,
#             limit=limit // 2,
#             with_payload=payload_selector,
#             score_threshold=0.15
#         )
#         if hasattr(hits, 'points'): hits = hits.points
#
#         for hit in hits:
#             loc = hit.payload.get('location')
#             # Fallback: å¦‚æœ location ç©ºï¼Œå°è¯•ä» geo_detail ç®—
#             if not loc:
#                 geo = hit.payload.get('geo_detail', {}).get('wgs84', {})
#                 if 'center' in geo:
#                     loc = {'lat': geo['center'][0], 'lon': geo['center'][1]}
#
#             if loc and 'lat' in loc:
#                 # åœ°å›¾ç»“æœåŠ æƒ 1.2
#                 points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=hit.score * 1.2))
#
#     except Exception as e:
#         print(f"Heatmap map error: {e}")
#
#     return points


import io
from typing import Optional, List

import numpy as np
from PIL import Image

from backend.app.core.config import settings
from backend.app.schema.search import SearchResultItem, SearchFilters, HeatmapPoint
from backend.app.utils.global_state import GlobalState

from qdrant_client import models


class SearchService:
    def __init__(self):
        self.MAP_COLLECTION = settings.MAP_COLLECTION
        self.DOC_COLLECTION = settings.DOC_COLLECTION

    # ==========================================================================
    #  æ ¸å¿ƒç®—æ³•: å½’ä¸€åŒ–ä¸è¾…åŠ©å‡½æ•°
    # ==========================================================================

    def _normalize_scores(self, results: List[SearchResultItem]) -> List[SearchResultItem]:
        """
        Z-Score å½’ä¸€åŒ– (Standardization)
        å…¬å¼: z = (x - Î¼) / Ïƒ
        ä½œç”¨: å°†ä¸åŒæ¨¡å‹çš„åˆ†æ•°æ˜ å°„åˆ°åŒä¸€ä¸ªæ ‡å‡†æ­£æ€åˆ†å¸ƒä¸Šï¼Œä½¿å®ƒä»¬å¯ä»¥ç›¸äº’æ¯”è¾ƒã€‚
        """
        if not results or len(results) < 2:
            return results

        # 1. æå–åˆ†æ•°
        scores = [r.score for r in results]
        mean = np.mean(scores)
        std = np.std(scores)

        # 2. é˜²å¾¡æ€§å¤„ç†ï¼šå¦‚æœæ ‡å‡†å·®ä¸º0 (æ‰€æœ‰åˆ†æ•°éƒ½ä¸€æ ·)ï¼Œæ— æ³•å½’ä¸€åŒ–
        if std == 0:
            return results

        # 3. æ‰§è¡Œå½’ä¸€åŒ–
        for r in results:
            r.score = (r.score - mean) / std

        return results

    def _build_qdrant_filters(self, filters: SearchFilters) -> Optional[models.Filter]:
        """æ„å»º Qdrant è¿‡æ»¤å™¨"""
        if not filters:
            return None

        conditions = []
        if filters.year_start is not None:
            conditions.append(models.FieldCondition(key="year", range=models.Range(gte=filters.year_start)))
        if filters.year_end is not None:
            conditions.append(models.FieldCondition(key="year", range=models.Range(lte=filters.year_end)))
        if filters.map_source:
            conditions.append(
                models.FieldCondition(key="source_image", match=models.MatchValue(value=filters.map_source)))
        if filters.geo_bbox and len(filters.geo_bbox) == 4:
            conditions.append(
                models.FieldCondition(
                    key="location",
                    geo_bounding_box=models.GeoBoundingBox(
                        bottom_right=models.GeoPoint(lon=filters.geo_bbox[2], lat=filters.geo_bbox[1]),
                        top_left=models.GeoPoint(lon=filters.geo_bbox[0], lat=filters.geo_bbox[3])
                    )
                )
            )
        return models.Filter(must=conditions) if conditions else None

    def _hits_to_results(self, hits, result_type: str, default_content: str = "") -> List[SearchResultItem]:
        """å°† Qdrant è¿”å›çš„åŸå§‹ hits è½¬æ¢ä¸ºç»Ÿä¸€çš„æ•°æ®ç»“æ„"""
        results = []
        if isinstance(hits, tuple): hits = hits[0]
        if hasattr(hits, 'points'): hits = hits.points
        if not hits: return results

        for hit in hits:
            if isinstance(hit, tuple) or not hasattr(hit, 'score'): continue

            payload = hit.payload or {}
            loc = payload.get('location', {})

            # å†…å®¹å±•ç¤ºé€»è¾‘
            content_preview = payload.get('content', '')[
                              :200] + "..." if result_type == "document" else f"{default_content} ({payload.get('year', 'Unknown')})"

            item = SearchResultItem(
                id=str(hit.id),
                score=hit.score,
                year=payload.get('year', 0),
                lat=loc.get('lat', 0.0),
                lng=loc.get('lon', 0.0),
                source_dataset=payload.get('source_dataset') or payload.get('source_image') or 'Unknown',
                content=content_preview,
                fullData=payload,
                type=result_type,
                pixel_coords=payload.get('pixel_coords'),
                image_source=payload.get('source_image'),
                geo_polygon=payload.get('geo_detail')
            )
            print(item)
            results.append(item)
        return results

    # ==========================================================================
    #  åŠŸèƒ½ 1: æ–‡æœ¬æ··åˆæœç´¢ (Text -> Text & Image)
    # ==========================================================================

    def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
        SearchResultItem]:
        """
        å®ç°é€»è¾‘ï¼š
        1. åˆ†åˆ«è·å– Document (æ–‡æœæ–‡) å’Œ Map (æ–‡æœå›¾) ç»“æœã€‚
        2. ä½¿ç”¨å„è‡ªçš„â€œç»å¯¹é˜ˆå€¼â€è¿‡æ»¤æ‰æ— å…³ç»“æœã€‚
        3. å¯¹ä¸¤ç»„ç»“æœåˆ†åˆ«è¿›è¡Œ Z-Score å½’ä¸€åŒ–ã€‚
        4. åˆå¹¶ç»“æœã€‚
        5. ä½¿ç”¨â€œç›¸å¯¹é˜ˆå€¼â€ (Z-Score > 0) å†æ¬¡è¿‡æ»¤ï¼Œä¿ç•™é«˜äºå¹³å‡æ°´å¹³çš„ç»“æœã€‚
        6. æ’åºå¹¶è¿”å›ã€‚
        """
        client = GlobalState.get_db()
        q_filter = self._build_qdrant_filters(filters)

        # --- é…ç½®å‚æ•° ---
        DOC_MIN_SCORE = 0.45  # æ–‡æ¡£ç»å¯¹é˜ˆå€¼ (MiniLM)
        MAP_MIN_SCORE = 0.18  # åœ°å›¾ç»å¯¹é˜ˆå€¼ (CLIP/PE)
        Z_SCORE_THRESHOLD = 0  # ç›¸å¯¹é˜ˆå€¼ (æ ‡å‡†å·®)ï¼Œè®¾ä¸º 0 è¡¨ç¤ºåªå–å¹³å‡åˆ†ä»¥ä¸Šçš„ï¼Œ-0.5 è¡¨ç¤ºç¨å®½å®¹ä¸€ç‚¹

        doc_results = []
        map_results = []

        # 1. æœæ–‡æ¡£ (MiniLM)
        try:
            text_model = GlobalState.get_text_model()
            text_vec = text_model.encode(query).tolist()

            hits_doc = client.query_points(
                collection_name=self.DOC_COLLECTION,
                query=text_vec,
                using="text_vector",
                query_filter=q_filter,
                limit=limit * 2,  # å¤šå–ä¸€å€ç”¨äºåç»­ç­›é€‰
                with_payload=True
            )
            raw_docs = self._hits_to_results(hits_doc, "document")
            # ğŸ›¡ï¸ ç»å¯¹é˜ˆå€¼è¿‡æ»¤
            doc_results = [r for r in raw_docs if r.score > DOC_MIN_SCORE]
        except Exception as e:
            print(f"âš ï¸ Doc search failed: {e}")

        # 2. æœåœ°å›¾ (PE/CLIP)
        try:
            pe_model = GlobalState.get_pe_model()
            pe_vec = pe_model.extract_text_features(query)
            if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
            if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]

            hits_map = client.query_points(
                collection_name=self.MAP_COLLECTION,
                query=pe_vec,
                # maps é›†åˆé»˜è®¤å‘é‡å°±æ˜¯è§†è§‰å‘é‡
                query_filter=q_filter,
                limit=limit * 2,
                with_payload=True
            )
            raw_maps = self._hits_to_results(hits_map, "map_tile", "Map Fragment")
            # ğŸ›¡ï¸ ç»å¯¹é˜ˆå€¼è¿‡æ»¤
            map_results = [r for r in raw_maps if r.score > MAP_MIN_SCORE]
        except Exception as e:
            print(f"âš ï¸ Map search failed: {e}")

        # --- 3. ç‹¬ç«‹å½’ä¸€åŒ– (å…³é”®æ­¥éª¤) ---
        # å¿…é¡»åˆ†å¼€å½’ä¸€åŒ–ï¼Œå› ä¸ºä¸¤ä¸ªæ¨¡å‹çš„åŸå§‹åˆ†æ•°åˆ†å¸ƒå®Œå…¨ä¸åŒ
        if doc_results:
            doc_results = self._normalize_scores(doc_results)

        if map_results:
            map_results = self._normalize_scores(map_results)

        # --- 4. åˆå¹¶ä¸æœ€ç»ˆæ’åº ---
        all_results = doc_results + map_results

        # ğŸ›¡ï¸ ç›¸å¯¹é˜ˆå€¼è¿‡æ»¤ (Z-Score è¿‡æ»¤)
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†å‰”é™¤åœ¨å„è‡ªæ¨¡å‹ä¸­è¡¨ç°éƒ½å¾ˆå·®çš„â€œé•¿å°¾â€ç»“æœ
        final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]

        # æ’åº
        final_results.sort(key=lambda x: x.score, reverse=True)

        return final_results[:limit]

    # ==========================================================================
    #  åŠŸèƒ½ 2: å›¾ç‰‡æ··åˆæœç´¢ (Image -> Image & Text)
    # ==========================================================================

    def search_image(self, image_data: bytes, limit: int, threshold: float) -> List[SearchResultItem]:
        """
        å›¾ç‰‡æœç´¢åŒæ ·åº”ç”¨ Z-Score é€»è¾‘
        """
        client = GlobalState.get_db()
        pe_model = GlobalState.get_pe_model()

        try:
            image = Image.open(io.BytesIO(image_data))
            vector_list = pe_model.extract_image_features([image])[0].tolist()
        except Exception as e:
            raise ValueError(f"Invalid image: {e}")

        # å›¾ç‰‡æœç´¢é€šå¸¸ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œé˜ˆå€¼å¯ä»¥é«˜ä¸€ç‚¹
        MAP_IMG_MIN_SCORE = 0.40
        DOC_IMG_MIN_SCORE = 0.22
        Z_SCORE_THRESHOLD = 0  # å›¾ç‰‡æœç´¢ç»“æœè¾ƒå°‘ï¼Œç¨å¾®å®½å®¹ä¸€ç‚¹

        doc_results = []
        map_results = []

        # 1. æœåœ°å›¾ (å›¾æœå›¾)
        try:
            hits_map = client.query_points(
                collection_name=self.MAP_COLLECTION,
                query=vector_list,
                limit=limit * 2,
                with_payload=True
            )
            raw_maps = self._hits_to_results(hits_map, "map_tile", "Visual Match")
            map_results = [r for r in raw_maps if r.score > MAP_IMG_MIN_SCORE]
        except Exception as e:
            print(f"âš ï¸ Image->Map search failed: {e}")

        # 2. æœæ–‡æ¡£ (å›¾æœæ–‡ - éœ€æ–‡æ¡£åº“æœ‰ pe_vector)
        try:
            hits_doc = client.query_points(
                collection_name=self.DOC_COLLECTION,
                query=vector_list,
                using="pe_vector",
                limit=limit * 2,
                with_payload=True
            )

            raw_docs = self._hits_to_results(hits_doc, "document")
            doc_results = [r for r in raw_docs if r.score > DOC_IMG_MIN_SCORE]
        except Exception as e:
            print(f"âš ï¸ Image->Doc search failed: {e}")

        # 3. å½’ä¸€åŒ–ä¸åˆå¹¶
        if map_results: map_results = self._normalize_scores(map_results)
        if doc_results: doc_results = self._normalize_scores(doc_results)

        all_results = map_results + doc_results
        final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]

        final_results.sort(key=lambda x: x.score, reverse=True)

        return final_results[:limit]

    # ==========================================================================
    #  åŠŸèƒ½ 3: 3D çƒ­åŠ›å›¾æ•°æ®
    # ==========================================================================

    def get_heatmap_data(self, query: str, limit: int = 2000) -> List[HeatmapPoint]:
        """
        ä¸ºäº†æ€§èƒ½ï¼Œçƒ­åŠ›å›¾æ•°æ®ä¸è¿›è¡Œå¤æ‚çš„å½’ä¸€åŒ–ï¼Œç›´æ¥è¿”å›åŸå§‹åˆ†æ•°å³å¯ï¼Œ
        æˆ–è€…åªè¿›è¡Œç®€å•çš„ Min-Max ç¼©æ”¾ã€‚è¿™é‡Œä¿æŒåŸå§‹åˆ†æ•°ã€‚
        """
        client = GlobalState.get_db()
        payload_selector = models.PayloadSelectorInclude(include=["location"])
        points = []

        # æœæ–‡æ¡£
        try:
            text_model = GlobalState.get_text_model()
            vec = text_model.encode(query).tolist()
            hits = client.query_points(
                self.DOC_COLLECTION, query=vec, using="text_vector",
                limit=limit // 2, with_payload=payload_selector, score_threshold=0.35
            )
            if hasattr(hits, 'points'): hits = hits.points
            for h in hits:
                loc = h.payload.get('location')
                if loc: points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=h.score))
        except:
            pass

        # æœåœ°å›¾
        try:
            pe_model = GlobalState.get_pe_model()
            vec = pe_model.extract_text_features(query)[0].tolist()
            hits = client.query_points(
                self.MAP_COLLECTION, query=vec,
                limit=limit // 2, with_payload=payload_selector, score_threshold=0.20
            )
            if hasattr(hits, 'points'): hits = hits.points
            for h in hits:
                loc = h.payload.get('location')
                if not loc:
                    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ locï¼Œå°è¯•ä» geo_detail è·å–ä¸­å¿ƒç‚¹ (å¦‚æœæœ‰)
                    pass
                if loc: points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=h.score * 1.1))
        except:
            pass

        return points


# å¯¼å‡ºå•ä¾‹
search_service = SearchService()
