import io
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, List

import numpy as np
from PIL import Image

from backend.app.core.config import settings
from backend.app.repository.qdrant_repo import QdrantRepository
from backend.app.schema.search import SearchResultItem, SearchFilters, HeatmapPoint
from backend.app.utils.global_state import GlobalState

from qdrant_client import models


class SearchService:
    def __init__(self):
        self.MAP_COLLECTION = settings.MAP_COLLECTION
        self.DOC_COLLECTION = settings.DOC_COLLECTION
        self.repo = QdrantRepository()

    # ==========================================================================
    #  æ ¸å¿ƒç®—æ³•: å½’ä¸€åŒ–ä¸è¾…åŠ©å‡½æ•°
    # ==========================================================================

    def _normalize_scores(self, results: List[SearchResultItem]) -> List[SearchResultItem]:
        """
        Z-Score å½’ä¸€åŒ– (Standardization)
        å…¬å¼: z = (x - Î¼) / Ïƒ
        ä½œç”¨: å°†ä¸åŒæ¨¡å‹çš„åˆ†æ•°æ˜ å°„åˆ°åŒä¸€ä¸ªæ ‡å‡†æ­£æ€åˆ†å¸ƒä¸Šï¼Œä½¿å®ƒä»¬å¯ä»¥ç›¸äº’æ¯”è¾ƒã€‚
        """
        if not results or len(results) < 2:
            return results

        # 1. æå–åˆ†æ•°
        scores = [r.score for r in results]
        mean = np.mean(scores)
        std = np.std(scores)

        # 2. é˜²å¾¡æ€§å¤„ç†ï¼šå¦‚æœæ ‡å‡†å·®ä¸º0 (æ‰€æœ‰åˆ†æ•°éƒ½ä¸€æ ·)ï¼Œæ— æ³•å½’ä¸€åŒ–
        if std == 0:
            return results

        # 3. æ‰§è¡Œå½’ä¸€åŒ–
        for r in results:
            r.score = (r.score - mean) / std

        return results

    def _build_qdrant_filters(self, filters: SearchFilters) -> Optional[models.Filter]:
        """æ„å»º Qdrant è¿‡æ»¤å™¨"""
        if not filters:
            return None

        conditions = []
        if filters.year_start is not None:
            conditions.append(models.FieldCondition(key="year", range=models.Range(gte=filters.year_start)))
        if filters.year_end is not None:
            conditions.append(models.FieldCondition(key="year", range=models.Range(lte=filters.year_end)))
        if filters.map_source:
            conditions.append(
                models.FieldCondition(key="source_image", match=models.MatchValue(value=filters.map_source)))
        if filters.geo_bbox and len(filters.geo_bbox) == 4:
            conditions.append(
                models.FieldCondition(
                    key="location",
                    geo_bounding_box=models.GeoBoundingBox(
                        bottom_right=models.GeoPoint(lon=filters.geo_bbox[2], lat=filters.geo_bbox[1]),
                        top_left=models.GeoPoint(lon=filters.geo_bbox[0], lat=filters.geo_bbox[3])
                    )
                )
            )
        return models.Filter(must=conditions) if conditions else None

    def _hits_to_results(self, hits, result_type: str, default_content: str = "") -> List[SearchResultItem]:
        """å°† Qdrant è¿”å›çš„åŸå§‹ hits è½¬æ¢ä¸ºç»Ÿä¸€çš„æ•°æ®ç»“æ„"""
        results = []
        if isinstance(hits, tuple): hits = hits[0]
        if hasattr(hits, 'points'): hits = hits.points
        if not hits: return results

        for hit in hits:
            if isinstance(hit, tuple) or not hasattr(hit, 'score'): continue

            payload = hit.payload or {}
            loc = payload.get('location', {})

            # å†…å®¹å±•ç¤ºé€»è¾‘
            content_preview = payload.get('content', '')[
                              :200] + "..." if result_type == "document" else f"{default_content} ({payload.get('year', 'Unknown')})"

            item = SearchResultItem(
                id=str(hit.id),
                score=hit.score,
                year=payload.get('year', 0),
                lat=loc.get('lat', 0.0),
                lng=loc.get('lon', 0.0),
                source_dataset=payload.get('source_dataset') or payload.get('source_image') or 'Unknown',
                content=content_preview,
                fullData=payload,
                type=result_type,
                pixel_coords=payload.get('pixel_coords'),
                image_source=payload.get('source_image'),
                geo_polygon=payload.get('geo_detail')
            )
            # print(item)
            results.append(item)
        return results

    # ==========================================================================
    #  åŠŸèƒ½ 1: æ–‡æœ¬æ··åˆæœç´¢ (Text -> Text & Image)
    # ==========================================================================

    # def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
    #     SearchResultItem]:
    #     """
    #     å®ç°é€»è¾‘ï¼š
    #     1. åˆ†åˆ«è·å– Document (æ–‡æœæ–‡) å’Œ Map (æ–‡æœå›¾) ç»“æœã€‚
    #     2. ä½¿ç”¨å„è‡ªçš„â€œç»å¯¹é˜ˆå€¼â€è¿‡æ»¤æ‰æ— å…³ç»“æœã€‚
    #     3. å¯¹ä¸¤ç»„ç»“æœåˆ†åˆ«è¿›è¡Œ Z-Score å½’ä¸€åŒ–ã€‚
    #     4. åˆå¹¶ç»“æœã€‚
    #     5. ä½¿ç”¨â€œç›¸å¯¹é˜ˆå€¼â€ (Z-Score > 0) å†æ¬¡è¿‡æ»¤ï¼Œä¿ç•™é«˜äºå¹³å‡æ°´å¹³çš„ç»“æœã€‚
    #     6. æ’åºå¹¶è¿”å›ã€‚
    #     """
    #     client = GlobalState.get_db()
    #     q_filter = self._build_qdrant_filters(filters)
    #
    #     # --- é…ç½®å‚æ•° ---
    #     DOC_MIN_SCORE = 0.45  # æ–‡æ¡£ç»å¯¹é˜ˆå€¼ (MiniLM)
    #     MAP_MIN_SCORE = 0.18  # åœ°å›¾ç»å¯¹é˜ˆå€¼ (CLIP/PE)
    #     Z_SCORE_THRESHOLD = 0  # ç›¸å¯¹é˜ˆå€¼ (æ ‡å‡†å·®)ï¼Œè®¾ä¸º 0 è¡¨ç¤ºåªå–å¹³å‡åˆ†ä»¥ä¸Šçš„ï¼Œ-0.5 è¡¨ç¤ºç¨å®½å®¹ä¸€ç‚¹
    #
    #     doc_results = []
    #     map_results = []
    #
    #     # 1. æœæ–‡æ¡£ (MiniLM)
    #     try:
    #         text_model = GlobalState.get_text_model()
    #         text_vec = text_model.encode(query).tolist()
    #
    #         hits_doc = client.query_points(
    #             collection_name=self.DOC_COLLECTION,
    #             query=text_vec,
    #             using="text_vector",
    #             query_filter=q_filter,
    #             limit=limit * 2,  # å¤šå–ä¸€å€ç”¨äºåç»­ç­›é€‰
    #             with_payload=True
    #         )
    #         raw_docs = self._hits_to_results(hits_doc, "document")
    #         # ğŸ›¡ï¸ ç»å¯¹é˜ˆå€¼è¿‡æ»¤
    #         doc_results = [r for r in raw_docs if r.score > DOC_MIN_SCORE]
    #     except Exception as e:
    #         print(f"âš ï¸ Doc search failed: {e}")
    #
    #     # 2. æœåœ°å›¾ (PE/CLIP)
    #     try:
    #         pe_model = GlobalState.get_pe_model()
    #         pe_vec = pe_model.extract_text_features(query)
    #         if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
    #         if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
    #
    #         hits_map = client.query_points(
    #             collection_name=self.MAP_COLLECTION,
    #             query=pe_vec,
    #             # maps é›†åˆé»˜è®¤å‘é‡å°±æ˜¯è§†è§‰å‘é‡
    #             query_filter=q_filter,
    #             limit=limit * 2,
    #             with_payload=True
    #         )
    #         raw_maps = self._hits_to_results(hits_map, "map_tile", "Map Fragment")
    #         # ğŸ›¡ï¸ ç»å¯¹é˜ˆå€¼è¿‡æ»¤
    #         map_results = [r for r in raw_maps if r.score > MAP_MIN_SCORE]
    #     except Exception as e:
    #         print(f"âš ï¸ Map search failed: {e}")
    #
    #     # --- 3. ç‹¬ç«‹å½’ä¸€åŒ– (å…³é”®æ­¥éª¤) ---
    #     # å¿…é¡»åˆ†å¼€å½’ä¸€åŒ–ï¼Œå› ä¸ºä¸¤ä¸ªæ¨¡å‹çš„åŸå§‹åˆ†æ•°åˆ†å¸ƒå®Œå…¨ä¸åŒ
    #     if doc_results:
    #         doc_results = self._normalize_scores(doc_results)
    #
    #     if map_results:
    #         map_results = self._normalize_scores(map_results)
    #
    #     # --- 4. åˆå¹¶ä¸æœ€ç»ˆæ’åº ---
    #     all_results = doc_results + map_results
    #
    #     # ğŸ›¡ï¸ ç›¸å¯¹é˜ˆå€¼è¿‡æ»¤ (Z-Score è¿‡æ»¤)
    #     # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†å‰”é™¤åœ¨å„è‡ªæ¨¡å‹ä¸­è¡¨ç°éƒ½å¾ˆå·®çš„â€œé•¿å°¾â€ç»“æœ
    #     final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
    #
    #     # æ’åº
    #     final_results.sort(key=lambda x: x.score, reverse=True)
    #
    #     return final_results[:limit]
    # def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
    #     SearchResultItem]:
    #     t0 = time.time()
    #     timings = {}
    #
    #     def log_time(step_name, start_time):
    #         elapsed = time.time() - start_time
    #         timings[step_name] = elapsed
    #         print(f"â±ï¸ [Step: {step_name}] è€—æ—¶: {elapsed:.4f}s")
    #
    #     # --- 1. å‡†å¤‡å·¥ä½œ ---
    #     client = GlobalState.get_db()
    #     q_filter = self._build_qdrant_filters(filters)
    #
    #     DOC_MIN_SCORE = 0.45
    #     MAP_MIN_SCORE = 0.18
    #     Z_SCORE_THRESHOLD = 0
    #
    #     doc_results = []
    #     map_results = []
    #
    #     # --- 2. æœæ–‡æ¡£æµç¨‹ ---
    #     t_doc_start = time.time()
    #
    #     # 2.1 æ–‡æœ¬å‘é‡åŒ– (æ¨¡å‹æ¨ç†)
    #     t_model_doc = time.time()
    #     try:
    #         text_model = GlobalState.get_text_model()
    #         text_vec = text_model.encode(query).tolist()
    #     except Exception as e:
    #         text_vec = []
    #         print(f"Doc Model Error: {e}")
    #     log_time("Doc Embedding (æ¨¡å‹)", t_model_doc)
    #
    #     # 2.2 æ–‡æ¡£æ£€ç´¢ (Qdrant ç½‘ç»œ I/O)
    #     t_search_doc = time.time()
    #     try:
    #         if text_vec:
    #             hits_doc = client.query_points(
    #                 collection_name=self.DOC_COLLECTION,
    #                 query=text_vec,
    #                 using="text_vector",
    #                 query_filter=q_filter,
    #                 limit=limit * 2,
    #                 with_payload=True
    #             )
    #             raw_docs = self._hits_to_results(hits_doc, "document")
    #             doc_results = [r for r in raw_docs if r.score > DOC_MIN_SCORE]
    #     except Exception as e:
    #         print(f"Doc Search Error: {e}")
    #     log_time("Doc Qdrant Search (IO)", t_search_doc)
    #
    #     log_time("--> æ–‡æ¡£æœç´¢æ€»è€—æ—¶", t_doc_start)
    #     print("-" * 30)
    #
    #     # --- 3. æœåœ°å›¾æµç¨‹ ---
    #     t_map_start = time.time()
    #
    #     # 3.1 åœ°å›¾å‘é‡åŒ– (æ¨¡å‹æ¨ç†)
    #     t_model_map = time.time()
    #     try:
    #         pe_model = GlobalState.get_pe_model()
    #         pe_vec = pe_model.extract_text_features(query)
    #         if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
    #         if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
    #     except Exception as e:
    #         pe_vec = []
    #         print(f"Map Model Error: {e}")
    #     log_time("Map Embedding (æ¨¡å‹)", t_model_map)
    #
    #     # 3.2 åœ°å›¾æ£€ç´¢ (Qdrant ç½‘ç»œ I/O)
    #     t_search_map = time.time()
    #     try:
    #         if pe_vec:
    #             hits_map = client.query_points(
    #                 collection_name=self.MAP_COLLECTION,
    #                 query=pe_vec,
    #                 query_filter=q_filter,
    #                 limit=limit * 2,
    #                 with_payload=True
    #             )
    #             raw_maps = self._hits_to_results(hits_map, "map_tile", "Map Fragment")
    #             map_results = [r for r in raw_maps if r.score > MAP_MIN_SCORE]
    #     except Exception as e:
    #         print(f"Map Search Error: {e}")
    #     log_time("Map Qdrant Search (IO)", t_search_map)
    #
    #     log_time("--> åœ°å›¾æœç´¢æ€»è€—æ—¶", t_map_start)
    #     print("-" * 30)
    #
    #     # --- 4. å½’ä¸€åŒ–ä¸åå¤„ç† ---
    #     t_process = time.time()
    #     if doc_results: doc_results = self._normalize_scores(doc_results)
    #     if map_results: map_results = self._normalize_scores(map_results)
    #     all_results = doc_results + map_results
    #     final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
    #     final_results.sort(key=lambda x: x.score, reverse=True)
    #     log_time("Normalization & Sort", t_process)
    #
    #     # --- æ€»ç»“æŠ¥å‘Š ---
    #     total_time = time.time() - t0
    #     print("\n" + "=" * 40)
    #     print(f"ğŸ“Š æ€§èƒ½è¯Šæ–­æŠ¥å‘Š (æ€»è€—æ—¶: {total_time:.4f}s)")
    #     print(f"1. æ–‡æœ¬æ¨¡å‹è®¡ç®—: {timings.get('Doc Embedding (æ¨¡å‹)', 0):.4f}s")
    #     print(f"2. æ–‡æ¡£ IO è€—æ—¶: {timings.get('Doc Qdrant Search (IO)', 0):.4f}s")
    #     print(f"3. åœ°å›¾æ¨¡å‹è®¡ç®—: {timings.get('Map Embedding (æ¨¡å‹)', 0):.4f}s")
    #     print(f"4. åœ°å›¾ IO è€—æ—¶: {timings.get('Map Qdrant Search (IO)', 0):.4f}s")
    #     print("=" * 40 + "\n")
    #
    #     return final_results[:limit]

    # def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
    #     SearchResultItem]:
    #     """
    #     æé€Ÿä¼˜åŒ–ç‰ˆï¼šå¹¶è¡Œæ‰§è¡Œ + Payload ç˜¦èº« + æœåŠ¡ç«¯è¿‡æ»¤
    #     """
    #     client = GlobalState.get_db()
    #     q_filter = self._build_qdrant_filters(filters)
    #
    #     # --- é…ç½®å‚æ•° ---
    #     DOC_MIN_SCORE = 0.45
    #     MAP_MIN_SCORE = 0.18
    #     Z_SCORE_THRESHOLD = 0
    #
    #     # å…³é”®ä¼˜åŒ– 1: å®šä¹‰è½»é‡çº§ Payload è¿‡æ»¤å™¨
    #     # æ’é™¤æ‰é‚£äº›å·¨å¤§çš„å­—æ®µï¼Œåªå–åˆ—è¡¨å±•ç¤ºéœ€è¦çš„æ ¸å¿ƒå­—æ®µ
    #     # å¦‚æœå‰ç«¯ç‚¹å‡»è¯¦æƒ…éœ€è¦å®Œæ•´æ•°æ®ï¼Œå»ºè®®ç”¨ ID å†å»è°ƒä¸€æ¬¡ retrieve æ¥å£ï¼Œè€Œä¸æ˜¯åœ¨æœç´¢åˆ—è¡¨é‡Œå…¨æ‹‰å›æ¥
    #     # payload_selector = models.PayloadSelectorInclude(
    #     #     include=["year", "location", "source_dataset", "source_image", "content"]  # æ³¨æ„ï¼šæ ¹æ®æƒ…å†µ content ä¹Ÿå¯ä»¥æˆªæ–­æˆ–æ’é™¤
    #     # )
    #     # æˆ–è€…ä½¿ç”¨ Exclude æ¨¡å¼ (æ¨èï¼Œæ›´å®‰å…¨):
    #     payload_selector = models.PayloadSelectorExclude(
    #         exclude=["geo_detail", "full_metadata", "pixel_coords"]
    #     )
    #
    #     def fetch_docs():
    #         try:
    #             # [ä¼˜åŒ–å»ºè®®] å¦‚æœå¯èƒ½ï¼Œè¯·å°† GlobalState ä¸­çš„æ¨¡å‹ç§»è‡³ GPU: model.to('cuda')
    #             text_model = GlobalState.get_text_model()
    #             text_vec = text_model.encode(query).tolist()
    #
    #             hits = client.query_points(
    #                 collection_name=self.DOC_COLLECTION,
    #                 query=text_vec,
    #                 using="text_vector",
    #                 query_filter=q_filter,
    #                 limit=limit * 2,
    #                 with_payload=True,  # ğŸš€ ç˜¦èº«ï¼šåªæ‹‰å–å¿…è¦å­—æ®µ
    #                 score_threshold=DOC_MIN_SCORE,  # ğŸš€ è¿‡æ»¤ï¼šDBç«¯ç›´æ¥è¿‡æ»¤ä½åˆ†
    #                 search_params=models.SearchParams(
    #                     hnsw_ef=32,  # é»˜è®¤å¯èƒ½æ˜¯ null (è‡ªåŠ¨) æˆ–è¾ƒé«˜ã€‚
    #                     # è°ƒä½è¿™ä¸ªå€¼ (æ¯”å¦‚ 64 æˆ– 32) ä¼šæ˜¾è‘—æé€Ÿï¼Œä½†ä¼šç•¥å¾®é™ä½é•¿å°¾ç»“æœçš„å¬å›ç‡ã€‚
    #                     exact=False  # ç¡®ä¿å…³é—­ç²¾ç¡®æœç´¢
    #                 )
    #             )
    #             return self._hits_to_results(hits, "document")
    #         except Exception as e:
    #             print(f"âš ï¸ Doc Error: {e}")
    #             return []
    #
    #     def fetch_maps():
    #         try:
    #             pe_model = GlobalState.get_pe_model()
    #             pe_vec = pe_model.extract_text_features(query)
    #             if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
    #             if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
    #
    #             hits = client.query_points(
    #                 collection_name=self.MAP_COLLECTION,
    #                 query=pe_vec,
    #                 query_filter=q_filter,
    #                 limit=limit * 2,
    #                 with_payload=True,  # ğŸš€ ç˜¦èº«
    #                 score_threshold=MAP_MIN_SCORE,  # ğŸš€ è¿‡æ»¤
    #                 search_params=models.SearchParams(
    #                     hnsw_ef=128,  # é»˜è®¤å¯èƒ½æ˜¯ null (è‡ªåŠ¨) æˆ–è¾ƒé«˜ã€‚
    #                     # è°ƒä½è¿™ä¸ªå€¼ (æ¯”å¦‚ 64 æˆ– 32) ä¼šæ˜¾è‘—æé€Ÿï¼Œä½†ä¼šç•¥å¾®é™ä½é•¿å°¾ç»“æœçš„å¬å›ç‡ã€‚
    #                     exact=False  # ç¡®ä¿å…³é—­ç²¾ç¡®æœç´¢
    #                 )
    #             )
    #             return self._hits_to_results(hits, "map_tile", "Map Fragment")
    #         except Exception as e:
    #             print(f"âš ï¸ Map Error: {e}")
    #             return []
    #
    #     # å…³é”®ä¼˜åŒ– 2: å¹¶è¡Œæ‰§è¡Œ (ThreadPool)
    #     # ä¹‹å‰æ˜¯ä¸²è¡Œï¼š0.9 + 2.3 + 0.9 + 2.1 = 6.2s
    #     # ç°åœ¨æ˜¯å¹¶è¡Œï¼šMax(Docæµç¨‹, Mapæµç¨‹)
    #     doc_results, map_results = [], []
    #
    #     t_start = time.time()
    #     with ThreadPoolExecutor(max_workers=2) as executor:
    #         future_doc = executor.submit(fetch_docs)
    #         future_map = executor.submit(fetch_maps)
    #
    #         # ç­‰å¾…ç»“æœ
    #         doc_results = future_doc.result()
    #         map_results = future_map.result()
    #
    #     print(f"âš¡ Search completed in {time.time() - t_start:.4f}s")
    #
    #     # --- å½’ä¸€åŒ–ä¸åˆå¹¶ (é€»è¾‘ä¸å˜) ---
    #     if doc_results: self._normalize_scores(doc_results)
    #     if map_results: self._normalize_scores(map_results)
    #
    #     all_results = doc_results + map_results
    #     final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
    #     final_results.sort(key=lambda x: x.score, reverse=True)
    #
    #     return final_results[:limit]

    import time
    from concurrent.futures import ThreadPoolExecutor
    from qdrant_client import models

    # def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
    #     SearchResultItem]:
    #     """
    #     æé€Ÿä¼˜åŒ– V2ï¼šä¿®å¤ Payload Bug + æ¨ç†åˆ†ç¦» + å‚æ•°è°ƒä¼˜
    #     """
    #     t_start = time.time()
    #     client = GlobalState.get_db()  # ç¡®ä¿è¿™é‡Œè¿”å›çš„æ˜¯åŒä¸€ä¸ªå•ä¾‹ï¼Œä¸è¦æ¯æ¬¡é‡å»ºè¿æ¥
    #     q_filter = self._build_qdrant_filters(filters)
    #
    #     # --- 1. é…ç½®å‚æ•°è°ƒä¼˜ ---
    #     DOC_MIN_SCORE = 0.45
    #     MAP_MIN_SCORE = 0.18
    #     # è°ƒä½ hnsw_ef (æœç´¢æ—¶çš„æ¢ç´¢å¹¿åº¦)ã€‚å¯¹äº Top-K æ£€ç´¢ï¼Œ16-32 é€šå¸¸è¶³å¤Ÿï¼Œé€Ÿåº¦æå¿«ã€‚
    #     SEARCH_PARAMS = models.SearchParams(hnsw_ef=32, exact=False)
    #
    #     # --- 2. çœŸæ­£ç”Ÿæ•ˆçš„ Payload ç˜¦èº« ---
    #     # å®šä¹‰å¥½è¦åŒ…å«æˆ–æ’é™¤çš„å­—æ®µ
    #     payload_selector = models.PayloadSelectorExclude(
    #         exclude=["geo_detail", "full_metadata", "pixel_coords", "embedding"]  # ç¡®ä¿æ’é™¤ embedding æœ¬èº«ï¼Œå¦‚æœå­˜å‚¨äº†çš„è¯
    #     )
    #
    #     # --- 3. æ¨¡å‹æ¨ç† (CPU/GPU å¯†é›†å‹) ---
    #     # å»ºè®®ä¸²è¡Œæ‰§è¡Œæˆ–ç§»å‡ºæ­¤å‡½æ•°ã€‚å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼Œæ”¾åœ¨çº¿ç¨‹æ± é‡Œå®¹æ˜“å¯¼è‡´èµ„æºäº‰æŠ¢æˆ–æ˜¾å­˜æº¢å‡ºã€‚
    #     # æ‰“å°æ—¶é—´ä»¥å®šä½ç“¶é¢ˆ
    #     t_encode_start = time.time()
    #
    #     try:
    #         text_model = GlobalState.get_text_model()
    #         text_vec = text_model.encode(query).tolist()
    #     except Exception as e:
    #         print(f"âš ï¸ Text Model Error: {e}")
    #         return []
    #
    #     try:
    #         pe_model = GlobalState.get_pe_model()
    #         pe_vec = pe_model.extract_text_features(query)
    #         if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
    #         if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
    #     except Exception as e:
    #         print(f"âš ï¸ PE Model Error: {e}")
    #         pe_vec = None
    #
    #     print(f"â±ï¸ Encoding time: {time.time() - t_encode_start:.4f}s")  # ğŸ‘ˆ è§‚å¯Ÿè¿™é‡Œæ˜¯å¦å æ®äº† 2.5s ä¸­çš„å¤§éƒ¨åˆ†
    #
    #     # --- 4. å¹¶è¡ŒæŸ¥è¯¢ (I/O å¯†é›†å‹) ---
    #     # ç°åœ¨çº¿ç¨‹æ± é‡Œåªåšçº¯ç²¹çš„ç½‘ç»œ I/Oï¼Œæ•ˆç‡æœ€é«˜
    #
    #     def search_docs_io():
    #         if not text_vec: return []
    #         return client.query_points(
    #             collection_name=self.DOC_COLLECTION,
    #             query=text_vec,
    #             using="text_vector",
    #             query_filter=q_filter,
    #             limit=limit * 2,  # ç¨å¾®å¤šå–ä¸€ç‚¹ç”¨äºé‡æ’åº
    #             with_payload=True,  # âœ… å…³é”®ä¿®æ­£ï¼šä¼ å…¥ selector
    #             score_threshold=DOC_MIN_SCORE,
    #             search_params=SEARCH_PARAMS
    #         )
    #
    #     def search_maps_io():
    #         if pe_vec is None: return []
    #         return client.query_points(
    #             collection_name=self.MAP_COLLECTION,
    #             query=pe_vec,
    #             query_filter=q_filter,
    #             limit=limit * 2,
    #             with_payload=True,  # âœ… å…³é”®ä¿®æ­£ï¼šä¼ å…¥ selector
    #             score_threshold=MAP_MIN_SCORE,
    #             search_params=SEARCH_PARAMS
    #         )
    #
    #     doc_hits, map_hits = [], []
    #
    #     # å› ä¸ºåªæ˜¯å‘ç½‘ç»œè¯·æ±‚ï¼Œå¼€é”€æå°ï¼Œçº¿ç¨‹æ± æ‰çœŸæ­£å‘æŒ¥ä½œç”¨
    #     t_search_start = time.time()
    #     with ThreadPoolExecutor(max_workers=2) as executor:
    #         future_doc = executor.submit(search_docs_io)
    #         future_map = executor.submit(search_maps_io)
    #
    #         try:
    #             doc_hits = future_doc.result()
    #         except Exception as e:
    #             print(f"âš ï¸ Doc Search Error: {e}")
    #
    #         try:
    #             map_hits = future_map.result()
    #         except Exception as e:
    #             print(f"âš ï¸ Map Search Error: {e}")
    #
    #     print(f"â±ï¸ Qdrant I/O time: {time.time() - t_search_start:.4f}s")
    #
    #     # --- 5. ç»“æœå¤„ç† ---
    #     # (ä¿æŒåŸæœ‰é€»è¾‘)
    #     doc_results = self._hits_to_results(doc_hits, "document")
    #     map_results = self._hits_to_results(map_hits, "map_tile", "Map Fragment")
    #
    #     if doc_results: self._normalize_scores(doc_results)
    #     if map_results: self._normalize_scores(map_results)
    #
    #     all_results = doc_results + map_results
    #     # Z_SCORE_THRESHOLD å»ºè®®è®¾ä¸ºå…¨å±€å˜é‡
    #     final_results = [r for r in all_results if r.score > 0]
    #     final_results.sort(key=lambda x: x.score, reverse=True)
    #
    #     print(f"âš¡ Total Search completed in {time.time() - t_start:.4f}s")
    #     return final_results[:limit]
    def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
        SearchResultItem]:
        """
        ä¸šåŠ¡é€»è¾‘ï¼šæ–‡æœ¬æ··åˆæœç´¢
        """
        t_start = time.time()

        # --- 1. å‚æ•°å®šä¹‰ ---
        DOC_MIN_SCORE = 0.45
        MAP_MIN_SCORE = 0.18

        # --- 2. æ¨¡å‹æ¨ç† (CPU/GPU è®¡ç®—) ---
        t_encode = time.time()
        text_vec = []
        pe_vec = []

        try:
            # å‡è®¾è¿™äº› get_model æ“ä½œå¾ˆå¿«ï¼Œæˆ–è€…ä½ å¯ä»¥è¿›ä¸€æ­¥å°è£… ModelService
            text_vec = GlobalState.get_text_model().encode(query).tolist()
        except Exception as e:
            print(f"Text Model Error: {e}")

        try:
            pe_raw = GlobalState.get_pe_model().extract_text_features(query)
            # å¤„ç†ä¸€ä¸‹ç»´åº¦é—®é¢˜
            if hasattr(pe_raw, 'tolist'): pe_raw = pe_raw.tolist()
            if isinstance(pe_raw, list) and isinstance(pe_raw[0], list): pe_raw = pe_raw[0]
            pe_vec = pe_raw
        except Exception as e:
            print(f"PE Model Error: {e}")

        print(f"â±ï¸ Encoding: {time.time() - t_encode:.4f}s")

        # --- 3. å¹¶è¡Œæ•°æ®åº“æŸ¥è¯¢ (IO å¯†é›†å‹) ---
        # å®šä¹‰ä»»åŠ¡å‡½æ•°ï¼Œç›´æ¥è°ƒç”¨ Repository
        def fetch_docs():
            if not text_vec: return []
            # è°ƒç”¨ Repository
            return self.repo.search(
                collection_name=self.DOC_COLLECTION,
                query_vector=text_vec,
                filters=filters,
                limit=limit * 2,
                score_threshold=DOC_MIN_SCORE,
                vector_name="text_vector",
                hnsw_ef=32
            )

        def fetch_maps():
            if not pe_vec: return []
            # è°ƒç”¨ Repository
            return self.repo.search(
                collection_name=self.MAP_COLLECTION,
                query_vector=pe_vec,
                filters=filters,
                limit=limit * 2,
                score_threshold=MAP_MIN_SCORE,
                hnsw_ef=32
            )

        # æ‰§è¡Œå¹¶è¡Œ
        t_search = time.time()
        doc_hits, map_hits = [], []
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_doc = executor.submit(fetch_docs)
            future_map = executor.submit(fetch_maps)
            doc_hits = future_doc.result()
            map_hits = future_map.result()
        print(f"â±ï¸ IO Search: {time.time() - t_search:.4f}s")

        # --- 4. ç»“æœè½¬æ¢ä¸å½’ä¸€åŒ– ---
        doc_results = self._hits_to_results(doc_hits, "document")
        map_results = self._hits_to_results(map_hits, "map_tile", "Map Fragment")

        if doc_results: self._normalize_scores(doc_results)
        if map_results: self._normalize_scores(map_results)

        # --- 5. åˆå¹¶ä¸æ’åº ---
        all_results = doc_results + map_results
        # 0 è¡¨ç¤ºåªå–é«˜äºå¹³å‡åˆ†çš„
        final_results = [r for r in all_results if r.score > 0]
        final_results.sort(key=lambda x: x.score, reverse=True)

        print(f"âš¡ Total: {time.time() - t_start:.4f}s")
        return final_results[:limit]

    #
    # # ==========================================================================
    # #  åŠŸèƒ½ 2: å›¾ç‰‡æ··åˆæœç´¢ (Image -> Image & Text)
    # # ==========================================================================
    #
    # def search_image(self, image_data: bytes, limit: int, threshold: float) -> List[SearchResultItem]:
    #     """
    #     å›¾ç‰‡æœç´¢åŒæ ·åº”ç”¨ Z-Score é€»è¾‘
    #     """
    #     client = GlobalState.get_db()
    #     pe_model = GlobalState.get_pe_model()
    #
    #     try:
    #         image = Image.open(io.BytesIO(image_data))
    #         vector_list = pe_model.extract_image_features([image])[0].tolist()
    #     except Exception as e:
    #         raise ValueError(f"Invalid image: {e}")
    #
    #     # å›¾ç‰‡æœç´¢é€šå¸¸ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œé˜ˆå€¼å¯ä»¥é«˜ä¸€ç‚¹
    #     MAP_IMG_MIN_SCORE = 0.40
    #     DOC_IMG_MIN_SCORE = 0.22
    #     Z_SCORE_THRESHOLD = 0  # å›¾ç‰‡æœç´¢ç»“æœè¾ƒå°‘ï¼Œç¨å¾®å®½å®¹ä¸€ç‚¹
    #
    #     doc_results = []
    #     map_results = []
    #
    #     # 1. æœåœ°å›¾ (å›¾æœå›¾)
    #     try:
    #         hits_map = client.query_points(
    #             collection_name=self.MAP_COLLECTION,
    #             query=vector_list,
    #             limit=limit * 2,
    #             with_payload=True
    #         )
    #         raw_maps = self._hits_to_results(hits_map, "map_tile", "Visual Match")
    #         map_results = [r for r in raw_maps if r.score > MAP_IMG_MIN_SCORE]
    #     except Exception as e:
    #         print(f"âš ï¸ Image->Map search failed: {e}")
    #
    #     # 2. æœæ–‡æ¡£ (å›¾æœæ–‡ - éœ€æ–‡æ¡£åº“æœ‰ pe_vector)
    #     try:
    #         hits_doc = client.query_points(
    #             collection_name=self.DOC_COLLECTION,
    #             query=vector_list,
    #             using="pe_vector",
    #             limit=limit * 2,
    #             with_payload=True
    #         )
    #
    #         raw_docs = self._hits_to_results(hits_doc, "document")
    #         doc_results = [r for r in raw_docs if r.score > DOC_IMG_MIN_SCORE]
    #     except Exception as e:
    #         print(f"âš ï¸ Image->Doc search failed: {e}")
    #
    #     # 3. å½’ä¸€åŒ–ä¸åˆå¹¶
    #     if map_results: map_results = self._normalize_scores(map_results)
    #     if doc_results: doc_results = self._normalize_scores(doc_results)
    #
    #     all_results = map_results + doc_results
    #     final_results = [r for r in all_results if r.score > Z_SCORE_THRESHOLD]
    #
    #     final_results.sort(key=lambda x: x.score, reverse=True)
    #
    #     return final_results[:limit]
    def search_image(self, image_data: bytes, limit: int, threshold: float) -> List[SearchResultItem]:
        """
        å›¾ç‰‡æ··åˆæœç´¢ (Image -> Image & Text)
        é‡æ„ç‰ˆï¼šä½¿ç”¨ Repository å±‚ + å¹¶è¡ŒæŸ¥è¯¢
        """
        t_start = time.time()

        # --- 1. å‚æ•°å®šä¹‰ ---
        # å›¾ç‰‡æœç´¢é€šå¸¸ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œé˜ˆå€¼å¯ä»¥è®¾é«˜ä¸€ç‚¹
        MAP_IMG_MIN_SCORE = 0.40
        DOC_IMG_MIN_SCORE = 0.22


        # --- 2. å›¾åƒç‰¹å¾æå– (CPU å¯†é›†) ---
        t_encode = time.time()
        try:
            image = Image.open(io.BytesIO(image_data))
            pe_model = GlobalState.get_pe_model()
            # æå–å‘é‡å¹¶è½¬ä¸º list
            vector_list = pe_model.extract_image_features([image])[0].tolist()
        except Exception as e:
            # å›¾ç‰‡æ— æ•ˆç›´æ¥æŠ¥é”™æˆ–è¿”å›ç©ºï¼Œè§†ä¸šåŠ¡éœ€æ±‚å®š
            print(f"âš ï¸ Image Encoding Error: {e}")
            raise ValueError(f"Invalid image processing: {e}")
        print(f"â±ï¸ Image Encoding: {time.time() - t_encode:.4f}s")

        # --- 3. å¹¶è¡Œæ•°æ®åº“æŸ¥è¯¢ (IO å¯†é›†) ---
        def fetch_maps():
            # å›¾æœå›¾ (Visual Match)
            return self.repo.search(
                collection_name=self.MAP_COLLECTION,
                query_vector=vector_list,
                limit=limit * 2,
                score_threshold=MAP_IMG_MIN_SCORE,
                hnsw_ef=32
            )

        def fetch_docs():
            # å›¾æœæ–‡ (Visual -> Text Description)
            # æ³¨æ„ï¼šå¿…é¡»æŒ‡å®šä½¿ç”¨ "pe_vector" (è§†è§‰å¯¹é½å‘é‡)
            return self.repo.search(
                collection_name=self.DOC_COLLECTION,
                query_vector=vector_list,
                limit=limit * 2,
                score_threshold=DOC_IMG_MIN_SCORE,
                vector_name="pe_vector",
                hnsw_ef=32
            )

        t_search = time.time()
        map_hits, doc_hits = [], []

        # å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_map = executor.submit(fetch_maps)
            future_doc = executor.submit(fetch_docs)

            map_hits = future_map.result()
            doc_hits = future_doc.result()
        print(f"â±ï¸ IO Search: {time.time() - t_search:.4f}s")

        # --- 4. ç»“æœè½¬æ¢ ---
        # æ³¨æ„ç±»å‹æ ‡è¯†ï¼šmap_tile vs document
        map_results = self._hits_to_results(map_hits, "map_tile", "Visual Match")
        doc_results = self._hits_to_results(doc_hits, "document")

        # --- 5. å½’ä¸€åŒ– (Normalization) ---
        if map_results: self._normalize_scores(map_results)
        if doc_results: self._normalize_scores(doc_results)

        # --- 6. åˆå¹¶ä¸æ’åº ---
        all_results = map_results + doc_results

        # Z_SCORE_THRESHOLD = 0 (å–å¹³å‡åˆ†ä»¥ä¸Š)
        final_results = [r for r in all_results if r.score > 0]
        final_results.sort(key=lambda x: x.score, reverse=True)

        print(f"âš¡ Image Search Total: {time.time() - t_start:.4f}s")
        return final_results[:limit]

    # def search_text(self, query: str, limit: int, threshold: float, filters: Optional[SearchFilters] = None) -> List[
    #     SearchResultItem]:
    #     print("\nğŸ•µï¸â€â™€ï¸ [å¼€å§‹è¯Šæ–­] 3ç§’æ€§èƒ½ç“¶é¢ˆåˆ†æ...")
    #     start_time_all = time.time()
    #
    #     # ç”¨äºè®°å½•æ—¥å¿—çš„åˆ—è¡¨ï¼Œæœ€åç»Ÿä¸€æ‰“å°ï¼Œé¿å…printé˜»å¡å½±å“æµ‹é€Ÿ
    #     timeline = []
    #
    #     def record(thread_name, action, start_t):
    #         duration = time.time() - start_t
    #         timeline.append(f"[{time.time() - start_time_all:.3f}s] ğŸ§µ {thread_name}: {action} è€—æ—¶ {duration:.3f}s")
    #         return duration
    #
    #     client = GlobalState.get_db()
    #     q_filter = self._build_qdrant_filters(filters)
    #
    #     # --- ä»»åŠ¡ A: æœæ–‡æ¡£ ---
    #     def task_docs():
    #         t_start = time.time()
    #
    #         # 1. æ¨¡å‹ Embedding
    #         t0 = time.time()
    #         try:
    #             text_model = GlobalState.get_text_model()
    #             text_vec = text_model.encode(query).tolist()
    #             record("Docçº¿ç¨‹", "ğŸ§ æ¨¡å‹è®¡ç®—", t0)
    #         except Exception:
    #             text_vec = []
    #
    #         # 2. Qdrant æœç´¢
    #         t1 = time.time()
    #         try:
    #             hits = client.query_points(
    #                 collection_name=self.DOC_COLLECTION,
    #                 query=text_vec,
    #                 using="text_vector",
    #                 query_filter=q_filter,
    #                 limit=limit * 2,
    #                 with_payload=True  # ğŸ‘ˆ ç“¶é¢ˆå«Œç–‘ç‚¹ï¼šå…¨é‡ Payload
    #             )
    #             res = self._hits_to_results(hits, "document")
    #             record("Docçº¿ç¨‹", "â˜ï¸ç½‘ç»œIO(Search)", t1)
    #             return res
    #         except Exception as e:
    #             print(f"Doc Error: {e}")
    #             return []
    #
    #     # --- ä»»åŠ¡ B: æœåœ°å›¾ ---
    #     def task_maps():
    #         t_start = time.time()
    #
    #         # 1. æ¨¡å‹ Embedding
    #         t0 = time.time()
    #         try:
    #             pe_model = GlobalState.get_pe_model()
    #             # å‡è®¾è¿™é‡Œæœ‰ä¸€äº›å¤„ç†é€»è¾‘
    #             pe_vec = pe_model.extract_text_features(query)
    #             if hasattr(pe_vec, 'tolist'): pe_vec = pe_vec.tolist()
    #             if isinstance(pe_vec, list) and isinstance(pe_vec[0], list): pe_vec = pe_vec[0]
    #             record("Mapçº¿ç¨‹", "ğŸ§ æ¨¡å‹è®¡ç®—", t0)
    #         except Exception:
    #             pe_vec = []
    #
    #         # 2. Qdrant æœç´¢
    #         t1 = time.time()
    #         try:
    #             hits = client.query_points(
    #                 collection_name=self.MAP_COLLECTION,
    #                 query=pe_vec,
    #                 query_filter=q_filter,
    #                 limit=limit * 2,
    #                 with_payload=True  # ğŸ‘ˆ ç“¶é¢ˆå«Œç–‘ç‚¹ï¼šå…¨é‡ Payload
    #             )
    #             res = self._hits_to_results(hits, "map_tile", "Map Fragment")
    #             record("Mapçº¿ç¨‹", "â˜ï¸ç½‘ç»œIO(Search)", t1)
    #             return res
    #         except Exception as e:
    #             print(f"Map Error: {e}")
    #             return []
    #
    #     # --- å¹¶è¡Œæ‰§è¡Œ ---
    #     doc_results = []
    #     map_results = []
    #
    #     with ThreadPoolExecutor(max_workers=2) as executor:
    #         future_doc = executor.submit(task_docs)
    #         future_map = executor.submit(task_maps)
    #
    #         doc_results = future_doc.result()
    #         map_results = future_map.result()
    #
    #     total_time = time.time() - start_time_all
    #
    #     # --- æ‰“å°æŠ¥å‘Š ---
    #     print("\n" + "=" * 40)
    #     print("â±ï¸ æ—¶é—´è½´æŠ¥å‘Š:")
    #     for log in sorted(timeline):  # æŒ‰æ—¶é—´æ’åº
    #         print(log)
    #     print("-" * 40)
    #     print(f"ğŸ“‰ æ€»è€—æ—¶: {total_time:.3f}s")
    #     print("=" * 40 + "\n")
    #
    #     # (åŸæœ¬çš„åç»­å¤„ç†é€»è¾‘ï¼Œä¸ºäº†è·‘é€šæš‚ä¸”ä¿ç•™)
    #     all_results = doc_results + map_results
    #     return all_results[:limit]

    # ==========================================================================
    #  åŠŸèƒ½ 3: 3D çƒ­åŠ›å›¾æ•°æ®
    # ==========================================================================

    def get_heatmap_data(self, query: str, limit: int = 2000) -> List[HeatmapPoint]:
        """
        ä¸ºäº†æ€§èƒ½ï¼Œçƒ­åŠ›å›¾æ•°æ®ä¸è¿›è¡Œå¤æ‚çš„å½’ä¸€åŒ–ï¼Œç›´æ¥è¿”å›åŸå§‹åˆ†æ•°å³å¯ï¼Œ
        æˆ–è€…åªè¿›è¡Œç®€å•çš„ Min-Max ç¼©æ”¾ã€‚è¿™é‡Œä¿æŒåŸå§‹åˆ†æ•°ã€‚
        """
        client = GlobalState.get_db()
        payload_selector = models.PayloadSelectorInclude(include=["location"])
        points = []

        # æœæ–‡æ¡£
        try:
            text_model = GlobalState.get_text_model()
            vec = text_model.encode(query).tolist()
            hits = client.query_points(
                self.DOC_COLLECTION, query=vec, using="text_vector",
                limit=limit // 2, with_payload=payload_selector, score_threshold=0.35
            )
            if hasattr(hits, 'points'): hits = hits.points
            for h in hits:
                loc = h.payload.get('location')
                if loc: points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=h.score))
        except:
            pass

        # æœåœ°å›¾
        try:
            pe_model = GlobalState.get_pe_model()
            vec = pe_model.extract_text_features(query)[0].tolist()
            hits = client.query_points(
                self.MAP_COLLECTION, query=vec,
                limit=limit // 2, with_payload=payload_selector, score_threshold=0.20
            )
            if hasattr(hits, 'points'): hits = hits.points
            for h in hits:
                loc = h.payload.get('location')
                if not loc:
                    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ locï¼Œå°è¯•ä» geo_detail è·å–ä¸­å¿ƒç‚¹ (å¦‚æœæœ‰)
                    pass
                if loc: points.append(HeatmapPoint(lat=loc['lat'], lng=loc['lon'], score=h.score * 1.1))
        except:
            pass

        return points


# å¯¼å‡ºå•ä¾‹
search_service = SearchService()
